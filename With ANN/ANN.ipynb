{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd042e2",
   "metadata": {},
   "source": [
    "### required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f315e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f2b147",
   "metadata": {},
   "source": [
    "### read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66f4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.124801</td>\n",
       "      <td>-0.015187</td>\n",
       "      <td>-0.064338</td>\n",
       "      <td>0.262879</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.113799</td>\n",
       "      <td>-0.148811</td>\n",
       "      <td>-0.205616</td>\n",
       "      <td>-0.261122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134838</td>\n",
       "      <td>-0.332744</td>\n",
       "      <td>-0.286736</td>\n",
       "      <td>-0.023697</td>\n",
       "      <td>0.228608</td>\n",
       "      <td>-0.031569</td>\n",
       "      <td>0.172973</td>\n",
       "      <td>-0.195043</td>\n",
       "      <td>0.148164</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.097063</td>\n",
       "      <td>-0.014436</td>\n",
       "      <td>-0.095955</td>\n",
       "      <td>0.031967</td>\n",
       "      <td>-0.146378</td>\n",
       "      <td>-0.088851</td>\n",
       "      <td>-0.071764</td>\n",
       "      <td>-0.263454</td>\n",
       "      <td>-0.074410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041875</td>\n",
       "      <td>-0.240170</td>\n",
       "      <td>-0.471870</td>\n",
       "      <td>-0.224988</td>\n",
       "      <td>-0.054436</td>\n",
       "      <td>0.174568</td>\n",
       "      <td>-0.062333</td>\n",
       "      <td>-0.178137</td>\n",
       "      <td>0.119959</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.154197</td>\n",
       "      <td>-0.028579</td>\n",
       "      <td>0.232414</td>\n",
       "      <td>0.075446</td>\n",
       "      <td>-0.200116</td>\n",
       "      <td>-0.041843</td>\n",
       "      <td>-0.078516</td>\n",
       "      <td>-0.196742</td>\n",
       "      <td>-0.333876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152843</td>\n",
       "      <td>-0.259489</td>\n",
       "      <td>-0.065614</td>\n",
       "      <td>-0.161057</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.052930</td>\n",
       "      <td>-0.178147</td>\n",
       "      <td>-0.114418</td>\n",
       "      <td>0.059236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>0.034092</td>\n",
       "      <td>-0.020997</td>\n",
       "      <td>0.085887</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.034301</td>\n",
       "      <td>-0.078638</td>\n",
       "      <td>-0.288124</td>\n",
       "      <td>-0.111715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020707</td>\n",
       "      <td>-0.241568</td>\n",
       "      <td>-0.542977</td>\n",
       "      <td>-0.317555</td>\n",
       "      <td>0.122504</td>\n",
       "      <td>0.214234</td>\n",
       "      <td>0.082396</td>\n",
       "      <td>-0.205559</td>\n",
       "      <td>0.167533</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.115747</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>0.146651</td>\n",
       "      <td>0.025979</td>\n",
       "      <td>0.010059</td>\n",
       "      <td>-0.002839</td>\n",
       "      <td>-0.153150</td>\n",
       "      <td>-0.323944</td>\n",
       "      <td>-0.195107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027908</td>\n",
       "      <td>-0.319734</td>\n",
       "      <td>-0.354926</td>\n",
       "      <td>-0.300265</td>\n",
       "      <td>0.039217</td>\n",
       "      <td>0.172030</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>-0.246464</td>\n",
       "      <td>0.190492</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 352 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1         2         3         4         5  \\\n",
       "0           0 -0.124801 -0.015187 -0.064338  0.262879  0.019898  0.113799   \n",
       "1           1  0.097063 -0.014436 -0.095955  0.031967 -0.146378 -0.088851   \n",
       "2           2 -0.154197 -0.028579  0.232414  0.075446 -0.200116 -0.041843   \n",
       "3           3  0.007488  0.034092 -0.020997  0.085887 -0.005670 -0.034301   \n",
       "4           4 -0.115747 -0.000537  0.146651  0.025979  0.010059 -0.002839   \n",
       "\n",
       "          6         7         8  ...       341       342       343       344  \\\n",
       "0 -0.148811 -0.205616 -0.261122  ... -0.134838 -0.332744 -0.286736 -0.023697   \n",
       "1 -0.071764 -0.263454 -0.074410  ... -0.041875 -0.240170 -0.471870 -0.224988   \n",
       "2 -0.078516 -0.196742 -0.333876  ...  0.152843 -0.259489 -0.065614 -0.161057   \n",
       "3 -0.078638 -0.288124 -0.111715  ... -0.020707 -0.241568 -0.542977 -0.317555   \n",
       "4 -0.153150 -0.323944 -0.195107  ...  0.027908 -0.319734 -0.354926 -0.300265   \n",
       "\n",
       "        345       346       347       348       349  emotion  \n",
       "0  0.228608 -0.031569  0.172973 -0.195043  0.148164        3  \n",
       "1 -0.054436  0.174568 -0.062333 -0.178137  0.119959        4  \n",
       "2 -0.021888 -0.052930 -0.178147 -0.114418  0.059236        0  \n",
       "3  0.122504  0.214234  0.082396 -0.205559  0.167533        5  \n",
       "4  0.039217  0.172030  0.007291 -0.246464  0.190492        6  \n",
       "\n",
       "[5 rows x 352 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('/media/shashanks/3E0CD8C50CD878FB/CDAC/My work/CDAC PROJECTS/MACHINE LEARNING/Emotional-Support-Chatbot/Data Cleaning/output/train_set.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0fc2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.220822</td>\n",
       "      <td>-0.149309</td>\n",
       "      <td>0.229522</td>\n",
       "      <td>0.077280</td>\n",
       "      <td>-0.144864</td>\n",
       "      <td>-0.143393</td>\n",
       "      <td>-0.247127</td>\n",
       "      <td>-0.403835</td>\n",
       "      <td>-0.137673</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015193</td>\n",
       "      <td>0.010059</td>\n",
       "      <td>-0.165496</td>\n",
       "      <td>-0.331344</td>\n",
       "      <td>0.027144</td>\n",
       "      <td>0.260822</td>\n",
       "      <td>-0.001042</td>\n",
       "      <td>-0.168567</td>\n",
       "      <td>-0.002824</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.074997</td>\n",
       "      <td>-0.035592</td>\n",
       "      <td>0.045195</td>\n",
       "      <td>-0.086024</td>\n",
       "      <td>-0.063486</td>\n",
       "      <td>-0.201120</td>\n",
       "      <td>-0.044311</td>\n",
       "      <td>-0.106906</td>\n",
       "      <td>-0.109615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112806</td>\n",
       "      <td>-0.368792</td>\n",
       "      <td>-0.292717</td>\n",
       "      <td>-0.101794</td>\n",
       "      <td>-0.125418</td>\n",
       "      <td>0.149469</td>\n",
       "      <td>-0.136924</td>\n",
       "      <td>-0.197251</td>\n",
       "      <td>0.039831</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.013756</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>0.027155</td>\n",
       "      <td>-0.051509</td>\n",
       "      <td>0.036726</td>\n",
       "      <td>-0.088962</td>\n",
       "      <td>-0.172548</td>\n",
       "      <td>-0.086107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045207</td>\n",
       "      <td>-0.247204</td>\n",
       "      <td>-0.215097</td>\n",
       "      <td>-0.254347</td>\n",
       "      <td>0.080904</td>\n",
       "      <td>0.138323</td>\n",
       "      <td>-0.016690</td>\n",
       "      <td>-0.213705</td>\n",
       "      <td>0.153055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.021277</td>\n",
       "      <td>-0.115405</td>\n",
       "      <td>0.123839</td>\n",
       "      <td>0.133022</td>\n",
       "      <td>-0.040653</td>\n",
       "      <td>-0.115244</td>\n",
       "      <td>0.016122</td>\n",
       "      <td>-0.461963</td>\n",
       "      <td>-0.097009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013876</td>\n",
       "      <td>-0.351542</td>\n",
       "      <td>-0.341141</td>\n",
       "      <td>-0.276380</td>\n",
       "      <td>0.014562</td>\n",
       "      <td>0.239963</td>\n",
       "      <td>0.032924</td>\n",
       "      <td>-0.248423</td>\n",
       "      <td>0.211845</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.148447</td>\n",
       "      <td>-0.016515</td>\n",
       "      <td>-0.010261</td>\n",
       "      <td>0.046936</td>\n",
       "      <td>0.154715</td>\n",
       "      <td>-0.030947</td>\n",
       "      <td>0.010067</td>\n",
       "      <td>-0.072706</td>\n",
       "      <td>-0.163411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100257</td>\n",
       "      <td>-0.399971</td>\n",
       "      <td>-0.415081</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>-0.131081</td>\n",
       "      <td>0.033783</td>\n",
       "      <td>-0.089400</td>\n",
       "      <td>-0.246691</td>\n",
       "      <td>0.242625</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 352 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1         2         3         4         5  \\\n",
       "0           0  0.220822 -0.149309  0.229522  0.077280 -0.144864 -0.143393   \n",
       "1           1  0.074997 -0.035592  0.045195 -0.086024 -0.063486 -0.201120   \n",
       "2           2  0.013756  0.003683  0.010192  0.027155 -0.051509  0.036726   \n",
       "3           3 -0.021277 -0.115405  0.123839  0.133022 -0.040653 -0.115244   \n",
       "4           4 -0.148447 -0.016515 -0.010261  0.046936  0.154715 -0.030947   \n",
       "\n",
       "          6         7         8  ...       341       342       343       344  \\\n",
       "0 -0.247127 -0.403835 -0.137673  ... -0.015193  0.010059 -0.165496 -0.331344   \n",
       "1 -0.044311 -0.106906 -0.109615  ...  0.112806 -0.368792 -0.292717 -0.101794   \n",
       "2 -0.088962 -0.172548 -0.086107  ...  0.045207 -0.247204 -0.215097 -0.254347   \n",
       "3  0.016122 -0.461963 -0.097009  ... -0.013876 -0.351542 -0.341141 -0.276380   \n",
       "4  0.010067 -0.072706 -0.163411  ...  0.100257 -0.399971 -0.415081  0.006658   \n",
       "\n",
       "        345       346       347       348       349  emotion  \n",
       "0  0.027144  0.260822 -0.001042 -0.168567 -0.002824        7  \n",
       "1 -0.125418  0.149469 -0.136924 -0.197251  0.039831        3  \n",
       "2  0.080904  0.138323 -0.016690 -0.213705  0.153055        7  \n",
       "3  0.014562  0.239963  0.032924 -0.248423  0.211845        6  \n",
       "4 -0.131081  0.033783 -0.089400 -0.246691  0.242625        6  \n",
       "\n",
       "[5 rows x 352 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the testing set as well\n",
    "df_test = pd.read_csv('/media/shashanks/3E0CD8C50CD878FB/CDAC/My work/CDAC PROJECTS/MACHINE LEARNING/Emotional-Support-Chatbot/Data Cleaning/output/test_set.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6269b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the 'Unnamed: 0' col from both testing and training set\n",
    "df_train.drop('Unnamed: 0' , axis=1 , inplace=True)\n",
    "df_test.drop('Unnamed: 0' , axis=1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2f69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.124801</td>\n",
       "      <td>-0.015187</td>\n",
       "      <td>-0.064338</td>\n",
       "      <td>0.262879</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.113799</td>\n",
       "      <td>-0.148811</td>\n",
       "      <td>-0.205616</td>\n",
       "      <td>-0.261122</td>\n",
       "      <td>0.229175</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134838</td>\n",
       "      <td>-0.332744</td>\n",
       "      <td>-0.286736</td>\n",
       "      <td>-0.023697</td>\n",
       "      <td>0.228608</td>\n",
       "      <td>-0.031569</td>\n",
       "      <td>0.172973</td>\n",
       "      <td>-0.195043</td>\n",
       "      <td>0.148164</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.097063</td>\n",
       "      <td>-0.014436</td>\n",
       "      <td>-0.095955</td>\n",
       "      <td>0.031967</td>\n",
       "      <td>-0.146378</td>\n",
       "      <td>-0.088851</td>\n",
       "      <td>-0.071764</td>\n",
       "      <td>-0.263454</td>\n",
       "      <td>-0.074410</td>\n",
       "      <td>0.125038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041875</td>\n",
       "      <td>-0.240170</td>\n",
       "      <td>-0.471870</td>\n",
       "      <td>-0.224988</td>\n",
       "      <td>-0.054436</td>\n",
       "      <td>0.174568</td>\n",
       "      <td>-0.062333</td>\n",
       "      <td>-0.178137</td>\n",
       "      <td>0.119959</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.154197</td>\n",
       "      <td>-0.028579</td>\n",
       "      <td>0.232414</td>\n",
       "      <td>0.075446</td>\n",
       "      <td>-0.200116</td>\n",
       "      <td>-0.041843</td>\n",
       "      <td>-0.078516</td>\n",
       "      <td>-0.196742</td>\n",
       "      <td>-0.333876</td>\n",
       "      <td>-0.015943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152843</td>\n",
       "      <td>-0.259489</td>\n",
       "      <td>-0.065614</td>\n",
       "      <td>-0.161057</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.052930</td>\n",
       "      <td>-0.178147</td>\n",
       "      <td>-0.114418</td>\n",
       "      <td>0.059236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007488</td>\n",
       "      <td>0.034092</td>\n",
       "      <td>-0.020997</td>\n",
       "      <td>0.085887</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.034301</td>\n",
       "      <td>-0.078638</td>\n",
       "      <td>-0.288124</td>\n",
       "      <td>-0.111715</td>\n",
       "      <td>0.266632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020707</td>\n",
       "      <td>-0.241568</td>\n",
       "      <td>-0.542977</td>\n",
       "      <td>-0.317555</td>\n",
       "      <td>0.122504</td>\n",
       "      <td>0.214234</td>\n",
       "      <td>0.082396</td>\n",
       "      <td>-0.205559</td>\n",
       "      <td>0.167533</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.115747</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>0.146651</td>\n",
       "      <td>0.025979</td>\n",
       "      <td>0.010059</td>\n",
       "      <td>-0.002839</td>\n",
       "      <td>-0.153150</td>\n",
       "      <td>-0.323944</td>\n",
       "      <td>-0.195107</td>\n",
       "      <td>0.122883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027908</td>\n",
       "      <td>-0.319734</td>\n",
       "      <td>-0.354926</td>\n",
       "      <td>-0.300265</td>\n",
       "      <td>0.039217</td>\n",
       "      <td>0.172030</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>-0.246464</td>\n",
       "      <td>0.190492</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.124801 -0.015187 -0.064338  0.262879  0.019898  0.113799 -0.148811   \n",
       "1  0.097063 -0.014436 -0.095955  0.031967 -0.146378 -0.088851 -0.071764   \n",
       "2 -0.154197 -0.028579  0.232414  0.075446 -0.200116 -0.041843 -0.078516   \n",
       "3  0.007488  0.034092 -0.020997  0.085887 -0.005670 -0.034301 -0.078638   \n",
       "4 -0.115747 -0.000537  0.146651  0.025979  0.010059 -0.002839 -0.153150   \n",
       "\n",
       "          7         8         9  ...       341       342       343       344  \\\n",
       "0 -0.205616 -0.261122  0.229175  ... -0.134838 -0.332744 -0.286736 -0.023697   \n",
       "1 -0.263454 -0.074410  0.125038  ... -0.041875 -0.240170 -0.471870 -0.224988   \n",
       "2 -0.196742 -0.333876 -0.015943  ...  0.152843 -0.259489 -0.065614 -0.161057   \n",
       "3 -0.288124 -0.111715  0.266632  ... -0.020707 -0.241568 -0.542977 -0.317555   \n",
       "4 -0.323944 -0.195107  0.122883  ...  0.027908 -0.319734 -0.354926 -0.300265   \n",
       "\n",
       "        345       346       347       348       349  emotion  \n",
       "0  0.228608 -0.031569  0.172973 -0.195043  0.148164        3  \n",
       "1 -0.054436  0.174568 -0.062333 -0.178137  0.119959        4  \n",
       "2 -0.021888 -0.052930 -0.178147 -0.114418  0.059236        0  \n",
       "3  0.122504  0.214234  0.082396 -0.205559  0.167533        5  \n",
       "4  0.039217  0.172030  0.007291 -0.246464  0.190492        6  \n",
       "\n",
       "[5 rows x 351 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f77999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## separating target variable - 'emotion' from both testing and training test \n",
    "\n",
    "X_train = df_train.drop('emotion' , axis = 1) # emotion column not needed in X_train\n",
    "Y_train = df_train['emotion']\n",
    "\n",
    "X_test = df_test.drop('emotion' , axis = 1) # emotion column not needed in X_test\n",
    "Y_test = df_test['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8505f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "#   - since we converted our text into embedding via word2vec\n",
    "#   - to avoid any feature dominating other feature because of 1 being very big in number and other being too small\n",
    "#   - we will bring them to same scale\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a3f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.124801</td>\n",
       "      <td>-0.015187</td>\n",
       "      <td>-0.064338</td>\n",
       "      <td>0.262879</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.113799</td>\n",
       "      <td>-0.148811</td>\n",
       "      <td>-0.205616</td>\n",
       "      <td>-0.261122</td>\n",
       "      <td>0.229175</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033777</td>\n",
       "      <td>-0.134838</td>\n",
       "      <td>-0.332744</td>\n",
       "      <td>-0.286736</td>\n",
       "      <td>-0.023697</td>\n",
       "      <td>0.228608</td>\n",
       "      <td>-0.031569</td>\n",
       "      <td>0.172973</td>\n",
       "      <td>-0.195043</td>\n",
       "      <td>0.148164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.097063</td>\n",
       "      <td>-0.014436</td>\n",
       "      <td>-0.095955</td>\n",
       "      <td>0.031967</td>\n",
       "      <td>-0.146378</td>\n",
       "      <td>-0.088851</td>\n",
       "      <td>-0.071764</td>\n",
       "      <td>-0.263454</td>\n",
       "      <td>-0.074410</td>\n",
       "      <td>0.125038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217088</td>\n",
       "      <td>-0.041875</td>\n",
       "      <td>-0.240170</td>\n",
       "      <td>-0.471870</td>\n",
       "      <td>-0.224988</td>\n",
       "      <td>-0.054436</td>\n",
       "      <td>0.174568</td>\n",
       "      <td>-0.062333</td>\n",
       "      <td>-0.178137</td>\n",
       "      <td>0.119959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.154197</td>\n",
       "      <td>-0.028579</td>\n",
       "      <td>0.232414</td>\n",
       "      <td>0.075446</td>\n",
       "      <td>-0.200116</td>\n",
       "      <td>-0.041843</td>\n",
       "      <td>-0.078516</td>\n",
       "      <td>-0.196742</td>\n",
       "      <td>-0.333876</td>\n",
       "      <td>-0.015943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290032</td>\n",
       "      <td>0.152843</td>\n",
       "      <td>-0.259489</td>\n",
       "      <td>-0.065614</td>\n",
       "      <td>-0.161057</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.052930</td>\n",
       "      <td>-0.178147</td>\n",
       "      <td>-0.114418</td>\n",
       "      <td>0.059236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007488</td>\n",
       "      <td>0.034092</td>\n",
       "      <td>-0.020997</td>\n",
       "      <td>0.085887</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.034301</td>\n",
       "      <td>-0.078638</td>\n",
       "      <td>-0.288124</td>\n",
       "      <td>-0.111715</td>\n",
       "      <td>0.266632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060975</td>\n",
       "      <td>-0.020707</td>\n",
       "      <td>-0.241568</td>\n",
       "      <td>-0.542977</td>\n",
       "      <td>-0.317555</td>\n",
       "      <td>0.122504</td>\n",
       "      <td>0.214234</td>\n",
       "      <td>0.082396</td>\n",
       "      <td>-0.205559</td>\n",
       "      <td>0.167533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.115747</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>0.146651</td>\n",
       "      <td>0.025979</td>\n",
       "      <td>0.010059</td>\n",
       "      <td>-0.002839</td>\n",
       "      <td>-0.153150</td>\n",
       "      <td>-0.323944</td>\n",
       "      <td>-0.195107</td>\n",
       "      <td>0.122883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079748</td>\n",
       "      <td>0.027908</td>\n",
       "      <td>-0.319734</td>\n",
       "      <td>-0.354926</td>\n",
       "      <td>-0.300265</td>\n",
       "      <td>0.039217</td>\n",
       "      <td>0.172030</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>-0.246464</td>\n",
       "      <td>0.190492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 350 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.124801 -0.015187 -0.064338  0.262879  0.019898  0.113799 -0.148811   \n",
       "1  0.097063 -0.014436 -0.095955  0.031967 -0.146378 -0.088851 -0.071764   \n",
       "2 -0.154197 -0.028579  0.232414  0.075446 -0.200116 -0.041843 -0.078516   \n",
       "3  0.007488  0.034092 -0.020997  0.085887 -0.005670 -0.034301 -0.078638   \n",
       "4 -0.115747 -0.000537  0.146651  0.025979  0.010059 -0.002839 -0.153150   \n",
       "\n",
       "          7         8         9  ...       340       341       342       343  \\\n",
       "0 -0.205616 -0.261122  0.229175  ... -0.033777 -0.134838 -0.332744 -0.286736   \n",
       "1 -0.263454 -0.074410  0.125038  ...  0.217088 -0.041875 -0.240170 -0.471870   \n",
       "2 -0.196742 -0.333876 -0.015943  ...  0.290032  0.152843 -0.259489 -0.065614   \n",
       "3 -0.288124 -0.111715  0.266632  ...  0.060975 -0.020707 -0.241568 -0.542977   \n",
       "4 -0.323944 -0.195107  0.122883  ...  0.079748  0.027908 -0.319734 -0.354926   \n",
       "\n",
       "        344       345       346       347       348       349  \n",
       "0 -0.023697  0.228608 -0.031569  0.172973 -0.195043  0.148164  \n",
       "1 -0.224988 -0.054436  0.174568 -0.062333 -0.178137  0.119959  \n",
       "2 -0.161057 -0.021888 -0.052930 -0.178147 -0.114418  0.059236  \n",
       "3 -0.317555  0.122504  0.214234  0.082396 -0.205559  0.167533  \n",
       "4 -0.300265  0.039217  0.172030  0.007291 -0.246464  0.190492  \n",
       "\n",
       "[5 rows x 350 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed4b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.94175582, -0.56024697, -1.2578195 , ...,  2.68147837,\n",
       "         0.49981409,  0.37362686],\n",
       "       [ 0.89199809, -0.55286755, -1.49398576, ...,  0.54098213,\n",
       "         0.64431648,  0.11979313],\n",
       "       [-1.18471781, -0.69191264,  0.958747  , ..., -0.51254269,\n",
       "         1.18893346, -0.42667762],\n",
       "       ...,\n",
       "       [ 2.36450409, -1.04122209,  1.38262165, ...,  0.40603626,\n",
       "         0.67410327, -1.3822338 ],\n",
       "       [ 0.58140577,  0.47315817,  0.10898   , ...,  0.30772455,\n",
       "        -0.17305821, -0.52266295],\n",
       "       [-0.24003708, -0.38482368, -0.05765958, ..., -1.05043224,\n",
       "         0.46983446, -0.47859852]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets convert our numpy array into tensors\n",
    "\n",
    "# train set\n",
    "X_train_tensor = torch.tensor(X_train_scaled , dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train , dtype=torch.long)\n",
    "\n",
    "# test set\n",
    "X_test_tensor = torch.tensor(X_test_scaled , dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test , dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e61f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23535, 350])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49382c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23535])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0818ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now import the Dataset and DataLoader class\n",
    "# since our dataset is already preprocessed , we won't be writing any custom logic inside while loading the data\n",
    "# so lets directly wrap our tensors with TensorDataset class.\n",
    "\n",
    "from torch.utils.data import TensorDataset , DataLoader\n",
    "\n",
    "# wrapping the tensors into a dataset\n",
    "train_dataset = TensorDataset(X_train_tensor , Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor , Y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b8a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the DataLoader object to create batches \n",
    "\n",
    "train_loader = DataLoader(train_dataset , batch_size=32 , shuffle=True)\n",
    "test_loader = DataLoader(test_dataset , batch_size=32 , shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0464440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0121e+00,  1.6175e-03,  1.0273e+00,  ...,  6.0586e-01,\n",
      "         -1.3751e+00,  1.0464e+00],\n",
      "        [-8.0756e-01, -1.7452e-01, -5.0586e-01,  ..., -2.1757e-01,\n",
      "          2.6312e+00,  2.5450e+00],\n",
      "        [ 2.4382e-01,  8.1203e-01, -1.4741e-02,  ...,  4.0683e-01,\n",
      "          7.4504e-01,  2.6912e+00],\n",
      "        ...,\n",
      "        [-1.0553e+00, -6.1867e-01, -9.6466e-01,  ..., -1.1538e+00,\n",
      "          5.2536e-01,  4.4910e-02],\n",
      "        [ 1.1319e+00, -5.3610e-01,  1.2059e+00,  ...,  1.2184e-01,\n",
      "         -1.0059e+00,  1.8378e+00],\n",
      "        [-6.7688e-01, -6.6169e-01, -9.1652e-01,  ...,  1.3287e+00,\n",
      "          6.2274e-01,  3.5537e-01]])\n",
      "tensor([6, 3, 6, 0, 6, 3, 1, 4, 2, 0, 0, 0, 6, 2, 8, 2, 4, 7, 0, 1, 0, 6, 7, 3,\n",
      "        5, 2, 8, 1, 4, 3, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# lets see one mini batch \n",
    "for batch_x, batch_y in train_loader:\n",
    "    print(batch_x)\n",
    "    print(batch_y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f24001",
   "metadata": {},
   "source": [
    "### lets create our ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55358f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_dim = 350\n",
    "        hidden1_neurons = 256\n",
    "        hidden2_neurons = 128\n",
    "        hidden3_neurons = 64\n",
    "        output_dim = 9\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim , hidden1_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(hidden1_neurons , hidden2_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(hidden2_neurons , hidden3_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(hidden3_neurons , output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self , features):\n",
    "        #  just pass your features to the container , instead of passing o/p from each layer as input to next layer\n",
    "        # all the work will be done by container\n",
    "        out = self.network(features)\n",
    "\n",
    "        return out # return the output of sigmoid function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "epochs = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create model\n",
    "model = EmotionNeuralNet() \n",
    "\n",
    "# defining the loss function\n",
    "loss_function = nn.CrossEntropyLoss() # since it is a multiclass classification\n",
    "\n",
    "# defining the optimizer\n",
    "optimizer = optim.Adam(model.parameters() , lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 , loss : 1.615866308464952\n",
      "epoch : 2 , loss : 1.4745151684821949\n",
      "epoch : 3 , loss : 1.4289058020905308\n",
      "epoch : 4 , loss : 1.4005447583192068\n",
      "epoch : 5 , loss : 1.3824963935691377\n",
      "epoch : 6 , loss : 1.3606751212121353\n",
      "epoch : 7 , loss : 1.348894794916977\n",
      "epoch : 8 , loss : 1.3288567495410857\n",
      "epoch : 9 , loss : 1.3247935467440148\n",
      "epoch : 10 , loss : 1.3051263225953216\n",
      "epoch : 11 , loss : 1.2890124977768764\n",
      "epoch : 12 , loss : 1.2763917962008196\n",
      "epoch : 13 , loss : 1.265136237134752\n",
      "epoch : 14 , loss : 1.2569667937476998\n",
      "epoch : 15 , loss : 1.2541461126959843\n",
      "epoch : 16 , loss : 1.2380906840705352\n",
      "epoch : 17 , loss : 1.2377005815505981\n",
      "epoch : 18 , loss : 1.227808123211498\n",
      "epoch : 19 , loss : 1.2143232894494482\n",
      "epoch : 20 , loss : 1.2090129785077728\n",
      "epoch : 21 , loss : 1.199957240210927\n",
      "epoch : 22 , loss : 1.1860219249906747\n",
      "epoch : 23 , loss : 1.179110299228974\n",
      "epoch : 24 , loss : 1.1786457565005706\n",
      "epoch : 25 , loss : 1.1753573130168344\n",
      "epoch : 26 , loss : 1.1587444223139598\n",
      "epoch : 27 , loss : 1.152414546712585\n",
      "epoch : 28 , loss : 1.1448573702541383\n",
      "epoch : 29 , loss : 1.1440557469816313\n",
      "epoch : 30 , loss : 1.1341492110134468\n",
      "epoch : 31 , loss : 1.1334280004805846\n",
      "epoch : 32 , loss : 1.1257112174416366\n",
      "epoch : 33 , loss : 1.122315197373214\n",
      "epoch : 34 , loss : 1.1118088499035523\n",
      "epoch : 35 , loss : 1.11576339489092\n",
      "epoch : 36 , loss : 1.1003079343911097\n",
      "epoch : 37 , loss : 1.0985933057154003\n",
      "epoch : 38 , loss : 1.092802834170668\n",
      "epoch : 39 , loss : 1.0891355187834606\n",
      "epoch : 40 , loss : 1.0828593563612388\n",
      "epoch : 41 , loss : 1.079283480491975\n",
      "epoch : 42 , loss : 1.0717283657387546\n",
      "epoch : 43 , loss : 1.0700625182817811\n",
      "epoch : 44 , loss : 1.0604613603943067\n",
      "epoch : 45 , loss : 1.0607167765822099\n",
      "epoch : 46 , loss : 1.0645435360948676\n",
      "epoch : 47 , loss : 1.0538884104108033\n",
      "epoch : 48 , loss : 1.050803798696269\n",
      "epoch : 49 , loss : 1.0461649071101262\n",
      "epoch : 50 , loss : 1.0432796397286912\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    total_epoch_loss = 0\n",
    "\n",
    "    for batch_features , batch_labels in train_loader:\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        # calc loss\n",
    "        loss = loss_function(outputs , batch_labels)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad() # clearing gradients before going for backward pass \n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_epoch_loss / len(train_loader)\n",
    "    print(f\"epoch : {epoch + 1} , loss : {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143d1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmotionNeuralNet(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=350, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): Linear(in_features=64, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8386a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5285520054384772\n"
     ]
    }
   ],
   "source": [
    "# evaluation code\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features , batch_labels in test_loader:\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        _ , predicted = torch.max(outputs , dim=1)  # it returns the max values and their corresponding indices\n",
    "\n",
    "        total += batch_labels.shape[0] # adding the no. of rows which have been tested in 1 batch\n",
    "\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745c383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7077968982366688\n"
     ]
    }
   ],
   "source": [
    "# evaluation code for training set\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features , batch_labels in train_loader:\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        _ , predicted = torch.max(outputs , dim=1)  # it returns the max values and their corresponding indices\n",
    "\n",
    "        total += batch_labels.shape[0] # adding the no. of rows which have been tested in 1 batch\n",
    "\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9dffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BOOM!!!! Classic issue of overfitting identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801010e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even after using droput layer our training accuracy has reduced from 92% to 70% and our test accuracy is currently 52% \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f64cc",
   "metadata": {},
   "source": [
    "## Lets try with batch normalization now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ff16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_dim = 350\n",
    "        hidden1_neurons = 256\n",
    "        hidden2_neurons = 128\n",
    "        hidden3_neurons = 64\n",
    "        output_dim = 9\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim , hidden1_neurons),\n",
    "            nn.BatchNorm1d(hidden1_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(hidden1_neurons , hidden2_neurons),\n",
    "            nn.BatchNorm1d(hidden2_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(hidden2_neurons , hidden3_neurons),\n",
    "            nn.BatchNorm1d(hidden3_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(hidden3_neurons , output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self , features):\n",
    "        #  just pass your features to the container , instead of passing o/p from each layer as input to next layer\n",
    "        # all the work will be done by container\n",
    "        out = self.network(features)\n",
    "\n",
    "        return out # return the output of sigmoid function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "epochs = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa953c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create model\n",
    "model = EmotionNeuralNet() \n",
    "\n",
    "# defining the loss function\n",
    "loss_function = nn.CrossEntropyLoss() # since it is a multiclass classification\n",
    "\n",
    "# defining the optimizer\n",
    "optimizer = optim.Adam(model.parameters() , lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7382af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 , loss : 1.6269848403399405\n",
      "epoch : 2 , loss : 1.487717751737522\n",
      "epoch : 3 , loss : 1.4546389817057743\n",
      "epoch : 4 , loss : 1.421006082435665\n",
      "epoch : 5 , loss : 1.4049203760274078\n",
      "epoch : 6 , loss : 1.3852643185173688\n",
      "epoch : 7 , loss : 1.3789928531193214\n",
      "epoch : 8 , loss : 1.3612453557712876\n",
      "epoch : 9 , loss : 1.3490395417194003\n",
      "epoch : 10 , loss : 1.3371225980960804\n",
      "epoch : 11 , loss : 1.3303683346704296\n",
      "epoch : 12 , loss : 1.3226775023276391\n",
      "epoch : 13 , loss : 1.3071592664751022\n",
      "epoch : 14 , loss : 1.3062308303525914\n",
      "epoch : 15 , loss : 1.290874792424881\n",
      "epoch : 16 , loss : 1.2849409271679495\n",
      "epoch : 17 , loss : 1.2842875080912008\n",
      "epoch : 18 , loss : 1.2797396969374106\n",
      "epoch : 19 , loss : 1.2626012590430353\n",
      "epoch : 20 , loss : 1.2577048249380745\n",
      "epoch : 21 , loss : 1.2522654066915098\n",
      "epoch : 22 , loss : 1.2413520999252796\n",
      "epoch : 23 , loss : 1.2373022498643917\n",
      "epoch : 24 , loss : 1.2345989765194447\n",
      "epoch : 25 , loss : 1.228723805559718\n",
      "epoch : 26 , loss : 1.2203646022826433\n",
      "epoch : 27 , loss : 1.2236930570038764\n",
      "epoch : 28 , loss : 1.2096470878661976\n",
      "epoch : 29 , loss : 1.2036433705007252\n",
      "epoch : 30 , loss : 1.2061207916425623\n",
      "epoch : 31 , loss : 1.1995021465193967\n",
      "epoch : 32 , loss : 1.1831381398698557\n",
      "epoch : 33 , loss : 1.1859635539838802\n",
      "epoch : 34 , loss : 1.1845221176905476\n",
      "epoch : 35 , loss : 1.178028101992348\n",
      "epoch : 36 , loss : 1.1715185777486667\n",
      "epoch : 37 , loss : 1.1716777219882477\n",
      "epoch : 38 , loss : 1.1606663335114717\n",
      "epoch : 39 , loss : 1.1650170849879151\n",
      "epoch : 40 , loss : 1.1555756012220746\n",
      "epoch : 41 , loss : 1.1532698476120182\n",
      "epoch : 42 , loss : 1.1532182871647503\n",
      "epoch : 43 , loss : 1.1476555495806362\n",
      "epoch : 44 , loss : 1.1483172382349554\n",
      "epoch : 45 , loss : 1.140128698280972\n",
      "epoch : 46 , loss : 1.1305254240236853\n",
      "epoch : 47 , loss : 1.1314157884891913\n",
      "epoch : 48 , loss : 1.1305309069221434\n",
      "epoch : 49 , loss : 1.1279216903545286\n",
      "epoch : 50 , loss : 1.1256549258756896\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    total_epoch_loss = 0\n",
    "\n",
    "    for batch_features , batch_labels in train_loader:\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        # calc loss\n",
    "        loss = loss_function(outputs , batch_labels)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad() # clearing gradients before going for backward pass \n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_epoch_loss / len(train_loader)\n",
    "    print(f\"epoch : {epoch + 1} , loss : {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd93d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmotionNeuralNet(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=350, out_features=256, bias=True)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.3, inplace=False)\n",
       "    (12): Linear(in_features=64, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e1a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5457171991842285\n"
     ]
    }
   ],
   "source": [
    "# evaluation code\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features , batch_labels in test_loader:\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        _ , predicted = torch.max(outputs , dim=1)  # it returns the max values and their corresponding indices\n",
    "\n",
    "        total += batch_labels.shape[0] # adding the no. of rows which have been tested in 1 batch\n",
    "\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12c1a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6995963458678564\n"
     ]
    }
   ],
   "source": [
    "# evaluation code for training set\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features , batch_labels in train_loader:\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        _ , predicted = torch.max(outputs , dim=1)  # it returns the max values and their corresponding indices\n",
    "\n",
    "        total += batch_labels.shape[0] # adding the no. of rows which have been tested in 1 batch\n",
    "\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bit improvement seen there , now test accuracy increased by 2% and train accuracy is almost same as earlier approx 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e38389",
   "metadata": {},
   "source": [
    "## lets try with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d24442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_dim = 350\n",
    "        hidden1_neurons = 256\n",
    "        hidden2_neurons = 128\n",
    "        hidden3_neurons = 64\n",
    "        output_dim = 9\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim , hidden1_neurons),\n",
    "            nn.BatchNorm1d(hidden1_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(hidden1_neurons , hidden2_neurons),\n",
    "            nn.BatchNorm1d(hidden2_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(hidden2_neurons , hidden3_neurons),\n",
    "            nn.BatchNorm1d(hidden3_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(hidden3_neurons , output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self , features):\n",
    "        #  just pass your features to the container , instead of passing o/p from each layer as input to next layer\n",
    "        # all the work will be done by container\n",
    "        out = self.network(features)\n",
    "\n",
    "        return out # return the output of sigmoid function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "epochs = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8348622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create model\n",
    "model = EmotionNeuralNet() \n",
    "\n",
    "# defining the loss function\n",
    "loss_function = nn.CrossEntropyLoss() # since it is a multiclass classification\n",
    "\n",
    "# defining the optimizer\n",
    "optimizer = optim.Adam(model.parameters() , lr=learning_rate,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405bd4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 , loss : 1.6334238120395204\n",
      "epoch : 2 , loss : 1.4882018524829461\n",
      "epoch : 3 , loss : 1.4542208355244086\n",
      "epoch : 4 , loss : 1.4305653180117193\n",
      "epoch : 5 , loss : 1.4133887716933438\n",
      "epoch : 6 , loss : 1.3889150324722994\n",
      "epoch : 7 , loss : 1.3810745705243037\n",
      "epoch : 8 , loss : 1.3727075160359559\n",
      "epoch : 9 , loss : 1.3645730589556953\n",
      "epoch : 10 , loss : 1.3433617243300313\n",
      "epoch : 11 , loss : 1.3341635993641356\n",
      "epoch : 12 , loss : 1.32723314906268\n",
      "epoch : 13 , loss : 1.3213754309260326\n",
      "epoch : 14 , loss : 1.3129978135390126\n",
      "epoch : 15 , loss : 1.3056014795666155\n",
      "epoch : 16 , loss : 1.3012015079350576\n",
      "epoch : 17 , loss : 1.2946297519880792\n",
      "epoch : 18 , loss : 1.2883484145707411\n",
      "epoch : 19 , loss : 1.282250797295052\n",
      "epoch : 20 , loss : 1.2759498620972685\n",
      "epoch : 21 , loss : 1.2687667502981166\n",
      "epoch : 22 , loss : 1.2611998969931966\n",
      "epoch : 23 , loss : 1.2594141885638237\n",
      "epoch : 24 , loss : 1.2537465291502683\n",
      "epoch : 25 , loss : 1.2534607475542503\n",
      "epoch : 26 , loss : 1.2492904478441114\n",
      "epoch : 27 , loss : 1.2431321491527816\n",
      "epoch : 28 , loss : 1.236532291640406\n",
      "epoch : 29 , loss : 1.2337579470451758\n",
      "epoch : 30 , loss : 1.232407971892668\n",
      "epoch : 31 , loss : 1.223970054284386\n",
      "epoch : 32 , loss : 1.2233583580540575\n",
      "epoch : 33 , loss : 1.2255507039473108\n",
      "epoch : 34 , loss : 1.2254170154261848\n",
      "epoch : 35 , loss : 1.216318859030371\n",
      "epoch : 36 , loss : 1.2122749447336663\n",
      "epoch : 37 , loss : 1.2136869561736998\n",
      "epoch : 38 , loss : 1.2095856755647971\n",
      "epoch : 39 , loss : 1.2041606179883946\n",
      "epoch : 40 , loss : 1.1962129567628321\n",
      "epoch : 41 , loss : 1.1945220323522454\n",
      "epoch : 42 , loss : 1.2043823104999636\n",
      "epoch : 43 , loss : 1.189968712951826\n",
      "epoch : 44 , loss : 1.1898160744458437\n",
      "epoch : 45 , loss : 1.183599366037094\n",
      "epoch : 46 , loss : 1.1873593925619903\n",
      "epoch : 47 , loss : 1.1843073398846646\n",
      "epoch : 48 , loss : 1.1806324433697306\n",
      "epoch : 49 , loss : 1.1845755108996578\n",
      "epoch : 50 , loss : 1.1786553600560064\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    total_epoch_loss = 0\n",
    "\n",
    "    for batch_features , batch_labels in train_loader:\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        # calc loss\n",
    "        loss = loss_function(outputs , batch_labels)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad() # clearing gradients before going for backward pass \n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_epoch_loss / len(train_loader)\n",
    "    print(f\"epoch : {epoch + 1} , loss : {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc00ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmotionNeuralNet(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=350, out_features=256, bias=True)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.3, inplace=False)\n",
       "    (12): Linear(in_features=64, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991eafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5402787219578518\n"
     ]
    }
   ],
   "source": [
    "# evaluation code\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features , batch_labels in test_loader:\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        _ , predicted = torch.max(outputs , dim=1)  # it returns the max values and their corresponding indices\n",
    "\n",
    "        total += batch_labels.shape[0] # adding the no. of rows which have been tested in 1 batch\n",
    "\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee3f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6825154025918845\n"
     ]
    }
   ],
   "source": [
    "# evaluation code for training set\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features , batch_labels in train_loader:\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        _ , predicted = torch.max(outputs , dim=1)  # it returns the max values and their corresponding indices\n",
    "\n",
    "        total += batch_labels.shape[0] # adding the no. of rows which have been tested in 1 batch\n",
    "\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa55c3",
   "metadata": {},
   "source": [
    "## Lets tune the hyper parameters using Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8c3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da4e8d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.124801</td>\n",
       "      <td>-0.015187</td>\n",
       "      <td>-0.064338</td>\n",
       "      <td>0.262879</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.113799</td>\n",
       "      <td>-0.148811</td>\n",
       "      <td>-0.205616</td>\n",
       "      <td>-0.261122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134838</td>\n",
       "      <td>-0.332744</td>\n",
       "      <td>-0.286736</td>\n",
       "      <td>-0.023697</td>\n",
       "      <td>0.228608</td>\n",
       "      <td>-0.031569</td>\n",
       "      <td>0.172973</td>\n",
       "      <td>-0.195043</td>\n",
       "      <td>0.148164</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.097063</td>\n",
       "      <td>-0.014436</td>\n",
       "      <td>-0.095955</td>\n",
       "      <td>0.031967</td>\n",
       "      <td>-0.146378</td>\n",
       "      <td>-0.088851</td>\n",
       "      <td>-0.071764</td>\n",
       "      <td>-0.263454</td>\n",
       "      <td>-0.074410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041875</td>\n",
       "      <td>-0.240170</td>\n",
       "      <td>-0.471870</td>\n",
       "      <td>-0.224988</td>\n",
       "      <td>-0.054436</td>\n",
       "      <td>0.174568</td>\n",
       "      <td>-0.062333</td>\n",
       "      <td>-0.178137</td>\n",
       "      <td>0.119959</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.154197</td>\n",
       "      <td>-0.028579</td>\n",
       "      <td>0.232414</td>\n",
       "      <td>0.075446</td>\n",
       "      <td>-0.200116</td>\n",
       "      <td>-0.041843</td>\n",
       "      <td>-0.078516</td>\n",
       "      <td>-0.196742</td>\n",
       "      <td>-0.333876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152843</td>\n",
       "      <td>-0.259489</td>\n",
       "      <td>-0.065614</td>\n",
       "      <td>-0.161057</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.052930</td>\n",
       "      <td>-0.178147</td>\n",
       "      <td>-0.114418</td>\n",
       "      <td>0.059236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>0.034092</td>\n",
       "      <td>-0.020997</td>\n",
       "      <td>0.085887</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.034301</td>\n",
       "      <td>-0.078638</td>\n",
       "      <td>-0.288124</td>\n",
       "      <td>-0.111715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020707</td>\n",
       "      <td>-0.241568</td>\n",
       "      <td>-0.542977</td>\n",
       "      <td>-0.317555</td>\n",
       "      <td>0.122504</td>\n",
       "      <td>0.214234</td>\n",
       "      <td>0.082396</td>\n",
       "      <td>-0.205559</td>\n",
       "      <td>0.167533</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.115747</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>0.146651</td>\n",
       "      <td>0.025979</td>\n",
       "      <td>0.010059</td>\n",
       "      <td>-0.002839</td>\n",
       "      <td>-0.153150</td>\n",
       "      <td>-0.323944</td>\n",
       "      <td>-0.195107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027908</td>\n",
       "      <td>-0.319734</td>\n",
       "      <td>-0.354926</td>\n",
       "      <td>-0.300265</td>\n",
       "      <td>0.039217</td>\n",
       "      <td>0.172030</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>-0.246464</td>\n",
       "      <td>0.190492</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 352 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1         2         3         4         5  \\\n",
       "0           0 -0.124801 -0.015187 -0.064338  0.262879  0.019898  0.113799   \n",
       "1           1  0.097063 -0.014436 -0.095955  0.031967 -0.146378 -0.088851   \n",
       "2           2 -0.154197 -0.028579  0.232414  0.075446 -0.200116 -0.041843   \n",
       "3           3  0.007488  0.034092 -0.020997  0.085887 -0.005670 -0.034301   \n",
       "4           4 -0.115747 -0.000537  0.146651  0.025979  0.010059 -0.002839   \n",
       "\n",
       "          6         7         8  ...       341       342       343       344  \\\n",
       "0 -0.148811 -0.205616 -0.261122  ... -0.134838 -0.332744 -0.286736 -0.023697   \n",
       "1 -0.071764 -0.263454 -0.074410  ... -0.041875 -0.240170 -0.471870 -0.224988   \n",
       "2 -0.078516 -0.196742 -0.333876  ...  0.152843 -0.259489 -0.065614 -0.161057   \n",
       "3 -0.078638 -0.288124 -0.111715  ... -0.020707 -0.241568 -0.542977 -0.317555   \n",
       "4 -0.153150 -0.323944 -0.195107  ...  0.027908 -0.319734 -0.354926 -0.300265   \n",
       "\n",
       "        345       346       347       348       349  emotion  \n",
       "0  0.228608 -0.031569  0.172973 -0.195043  0.148164        3  \n",
       "1 -0.054436  0.174568 -0.062333 -0.178137  0.119959        4  \n",
       "2 -0.021888 -0.052930 -0.178147 -0.114418  0.059236        0  \n",
       "3  0.122504  0.214234  0.082396 -0.205559  0.167533        5  \n",
       "4  0.039217  0.172030  0.007291 -0.246464  0.190492        6  \n",
       "\n",
       "[5 rows x 352 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('/media/shashanks/3E0CD8C50CD878FB/CDAC/My work/CDAC PROJECTS/MACHINE LEARNING/Emotional-Support-Chatbot/Data Cleaning/output/train_set.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c380963d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.220822</td>\n",
       "      <td>-0.149309</td>\n",
       "      <td>0.229522</td>\n",
       "      <td>0.077280</td>\n",
       "      <td>-0.144864</td>\n",
       "      <td>-0.143393</td>\n",
       "      <td>-0.247127</td>\n",
       "      <td>-0.403835</td>\n",
       "      <td>-0.137673</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015193</td>\n",
       "      <td>0.010059</td>\n",
       "      <td>-0.165496</td>\n",
       "      <td>-0.331344</td>\n",
       "      <td>0.027144</td>\n",
       "      <td>0.260822</td>\n",
       "      <td>-0.001042</td>\n",
       "      <td>-0.168567</td>\n",
       "      <td>-0.002824</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.074997</td>\n",
       "      <td>-0.035592</td>\n",
       "      <td>0.045195</td>\n",
       "      <td>-0.086024</td>\n",
       "      <td>-0.063486</td>\n",
       "      <td>-0.201120</td>\n",
       "      <td>-0.044311</td>\n",
       "      <td>-0.106906</td>\n",
       "      <td>-0.109615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112806</td>\n",
       "      <td>-0.368792</td>\n",
       "      <td>-0.292717</td>\n",
       "      <td>-0.101794</td>\n",
       "      <td>-0.125418</td>\n",
       "      <td>0.149469</td>\n",
       "      <td>-0.136924</td>\n",
       "      <td>-0.197251</td>\n",
       "      <td>0.039831</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.013756</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>0.027155</td>\n",
       "      <td>-0.051509</td>\n",
       "      <td>0.036726</td>\n",
       "      <td>-0.088962</td>\n",
       "      <td>-0.172548</td>\n",
       "      <td>-0.086107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045207</td>\n",
       "      <td>-0.247204</td>\n",
       "      <td>-0.215097</td>\n",
       "      <td>-0.254347</td>\n",
       "      <td>0.080904</td>\n",
       "      <td>0.138323</td>\n",
       "      <td>-0.016690</td>\n",
       "      <td>-0.213705</td>\n",
       "      <td>0.153055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.021277</td>\n",
       "      <td>-0.115405</td>\n",
       "      <td>0.123839</td>\n",
       "      <td>0.133022</td>\n",
       "      <td>-0.040653</td>\n",
       "      <td>-0.115244</td>\n",
       "      <td>0.016122</td>\n",
       "      <td>-0.461963</td>\n",
       "      <td>-0.097009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013876</td>\n",
       "      <td>-0.351542</td>\n",
       "      <td>-0.341141</td>\n",
       "      <td>-0.276380</td>\n",
       "      <td>0.014562</td>\n",
       "      <td>0.239963</td>\n",
       "      <td>0.032924</td>\n",
       "      <td>-0.248423</td>\n",
       "      <td>0.211845</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.148447</td>\n",
       "      <td>-0.016515</td>\n",
       "      <td>-0.010261</td>\n",
       "      <td>0.046936</td>\n",
       "      <td>0.154715</td>\n",
       "      <td>-0.030947</td>\n",
       "      <td>0.010067</td>\n",
       "      <td>-0.072706</td>\n",
       "      <td>-0.163411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100257</td>\n",
       "      <td>-0.399971</td>\n",
       "      <td>-0.415081</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>-0.131081</td>\n",
       "      <td>0.033783</td>\n",
       "      <td>-0.089400</td>\n",
       "      <td>-0.246691</td>\n",
       "      <td>0.242625</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 352 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1         2         3         4         5  \\\n",
       "0           0  0.220822 -0.149309  0.229522  0.077280 -0.144864 -0.143393   \n",
       "1           1  0.074997 -0.035592  0.045195 -0.086024 -0.063486 -0.201120   \n",
       "2           2  0.013756  0.003683  0.010192  0.027155 -0.051509  0.036726   \n",
       "3           3 -0.021277 -0.115405  0.123839  0.133022 -0.040653 -0.115244   \n",
       "4           4 -0.148447 -0.016515 -0.010261  0.046936  0.154715 -0.030947   \n",
       "\n",
       "          6         7         8  ...       341       342       343       344  \\\n",
       "0 -0.247127 -0.403835 -0.137673  ... -0.015193  0.010059 -0.165496 -0.331344   \n",
       "1 -0.044311 -0.106906 -0.109615  ...  0.112806 -0.368792 -0.292717 -0.101794   \n",
       "2 -0.088962 -0.172548 -0.086107  ...  0.045207 -0.247204 -0.215097 -0.254347   \n",
       "3  0.016122 -0.461963 -0.097009  ... -0.013876 -0.351542 -0.341141 -0.276380   \n",
       "4  0.010067 -0.072706 -0.163411  ...  0.100257 -0.399971 -0.415081  0.006658   \n",
       "\n",
       "        345       346       347       348       349  emotion  \n",
       "0  0.027144  0.260822 -0.001042 -0.168567 -0.002824        7  \n",
       "1 -0.125418  0.149469 -0.136924 -0.197251  0.039831        3  \n",
       "2  0.080904  0.138323 -0.016690 -0.213705  0.153055        7  \n",
       "3  0.014562  0.239963  0.032924 -0.248423  0.211845        6  \n",
       "4 -0.131081  0.033783 -0.089400 -0.246691  0.242625        6  \n",
       "\n",
       "[5 rows x 352 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the testing set as well\n",
    "df_test = pd.read_csv('/media/shashanks/3E0CD8C50CD878FB/CDAC/My work/CDAC PROJECTS/MACHINE LEARNING/Emotional-Support-Chatbot/Data Cleaning/output/test_set.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba783758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the 'Unnamed: 0' col from both testing and training set\n",
    "df_train.drop('Unnamed: 0' , axis=1 , inplace=True)\n",
    "df_test.drop('Unnamed: 0' , axis=1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c782fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## separating target variable - 'emotion' from both testing and training test \n",
    "\n",
    "X_train = df_train.drop('emotion' , axis = 1) # emotion column not needed in X_train\n",
    "Y_train = df_train['emotion']\n",
    "\n",
    "X_test = df_test.drop('emotion' , axis = 1) # emotion column not needed in X_test\n",
    "Y_test = df_test['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3060c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "#   - since we converted our text into embedding via word2vec\n",
    "#   - to avoid any feature dominating other feature because of 1 being very big in number and other being too small\n",
    "#   - we will bring them to same scale\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c69653f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets convert our numpy array into tensors\n",
    "\n",
    "# train set\n",
    "X_train_tensor = torch.tensor(X_train_scaled , dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train , dtype=torch.long)\n",
    "\n",
    "# test set\n",
    "X_test_tensor = torch.tensor(X_test_scaled , dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test , dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb9bcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now import the Dataset and DataLoader class\n",
    "# since our dataset is already preprocessed , we won't be writing any custom logic inside while loading the data\n",
    "# so lets directly wrap our tensors with TensorDataset class.\n",
    "\n",
    "from torch.utils.data import TensorDataset , DataLoader\n",
    "\n",
    "# wrapping the tensors into a dataset\n",
    "train_dataset = TensorDataset(X_train_tensor , Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor , Y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb01457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionNeuralNet(nn.Module):\n",
    "    def __init__(self , input_dim , output_dim , num_hidden_layers , neurons_per_layer , dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        for i in range(num_hidden_layers):\n",
    "\n",
    "            layers.append(nn.Linear(input_dim , neurons_per_layer))\n",
    "            layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = neurons_per_layer  # for the 2nd iteration , we need to update the input dim\n",
    "        \n",
    "        layers.append(nn.Linear(neurons_per_layer , output_dim))\n",
    "\n",
    "        self.model = nn.Sequential(*layers) # since Sequential needs all the inputs individually , so we need to unpack the list thats why *layers\n",
    "\n",
    "\n",
    "    def forward(self , features):\n",
    "        #  just pass your features to the container , instead of passing o/p from each layer as input to next layer\n",
    "        # all the work will be done by container\n",
    "        out = self.model(features)\n",
    "\n",
    "        return out # return the output of sigmoid function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c8deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# define objective function \n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # extract next hyperparamter values from the search space\n",
    "    num_hidden_layers = trial.suggest_int(\"num_hidden_layers\" , 1,5)\n",
    "    neurons_per_layer = trial.suggest_int(\"neurons_per_layers\",10,350,step = 10)\n",
    "    epochs = trial.suggest_int(\"epochs\" , 10,100,step=10)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\" , 1e-5 , 1e-1 , log=True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\" , 0.1 , 0.5 , step = 0.1)\n",
    "    batch_size = trial.suggest_categorical (\"batch_size\" , [16 , 32 , 64 , 128])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\" , ['Adam', 'SGD' , 'RMSProp'])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\" , 1e-5 , 1e-3 , log=True)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset , batch_size=batch_size , shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset , batch_size=batch_size , shuffle=False)\n",
    "\n",
    "    # model init\n",
    "    input_dim = 350\n",
    "    output_dim = 9\n",
    "\n",
    "    model = EmotionNeuralNet(input_dim , output_dim , num_hidden_layers , neurons_per_layer , dropout_rate)\n",
    "\n",
    "    # optimizer selection\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters() , lr = learning_rate , weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters() , lr = learning_rate , weight_decay=weight_decay)\n",
    "    else : # 'RMSprop'\n",
    "        optimizer = optim.RMSprop(model.parameters() , lr = learning_rate , weight_decay=weight_decay)\n",
    "        \n",
    "\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for batch_features , batch_labels in train_loader:\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(batch_features)\n",
    "\n",
    "            # calc loss\n",
    "            loss = loss_function(outputs , batch_labels)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad() # clearing gradients before going for backward pass \n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_features , batch_labels in test_loader:\n",
    "            outputs = model(batch_features)\n",
    "\n",
    "            _ , predicted = torch.max(outputs , dim=1)  # it returns the max values and their corresponding indices\n",
    "\n",
    "            total += batch_labels.shape[0] # adding the no. of rows which have been tested in 1 batch\n",
    "\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6db4d9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/shashanks/3E0CD8C50CD878FB/CDAC/My work/env_nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-06-24 09:04:23,738] A new study created in memory with name: no-name-d58c646f-2b94-448d-b634-c85c1ab5a2c1\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3438a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-24 09:04:56,482] Trial 0 finished with value: 0.4340584636301835 and parameters: {'num_hidden_layers': 1, 'neurons_per_layers': 120, 'epochs': 60, 'learning_rate': 0.00015293154986116932, 'dropout_rate': 0.2, 'batch_size': 128, 'optimizer': 'SGD', 'weight_decay': 5.533961441326209e-05}. Best is trial 0 with value: 0.4340584636301835.\n",
      "[I 2025-06-24 09:08:17,327] Trial 1 finished with value: 0.4862338545207342 and parameters: {'num_hidden_layers': 4, 'neurons_per_layers': 40, 'epochs': 90, 'learning_rate': 0.0038150536542624937, 'dropout_rate': 0.30000000000000004, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.0006080896704663652}. Best is trial 1 with value: 0.4862338545207342.\n",
      "[I 2025-06-24 09:09:17,687] Trial 2 finished with value: 0.5338205302515295 and parameters: {'num_hidden_layers': 3, 'neurons_per_layers': 50, 'epochs': 80, 'learning_rate': 9.805764084325251e-05, 'dropout_rate': 0.1, 'batch_size': 128, 'optimizer': 'Adam', 'weight_decay': 6.503188155951901e-05}. Best is trial 2 with value: 0.5338205302515295.\n",
      "[I 2025-06-24 09:10:47,895] Trial 3 finished with value: 0.5078178110129163 and parameters: {'num_hidden_layers': 5, 'neurons_per_layers': 170, 'epochs': 30, 'learning_rate': 1.1078099662270645e-05, 'dropout_rate': 0.1, 'batch_size': 32, 'optimizer': 'RMSProp', 'weight_decay': 0.00048395144653646285}. Best is trial 2 with value: 0.5338205302515295.\n",
      "[I 2025-06-24 09:11:43,840] Trial 4 finished with value: 0.18082936777702244 and parameters: {'num_hidden_layers': 4, 'neurons_per_layers': 40, 'epochs': 90, 'learning_rate': 2.2151651481418954e-05, 'dropout_rate': 0.2, 'batch_size': 128, 'optimizer': 'SGD', 'weight_decay': 0.0005044841099395278}. Best is trial 2 with value: 0.5338205302515295.\n",
      "[I 2025-06-24 09:12:28,368] Trial 5 finished with value: 0.5253229095853161 and parameters: {'num_hidden_layers': 2, 'neurons_per_layers': 210, 'epochs': 20, 'learning_rate': 0.0017422401038408397, 'dropout_rate': 0.30000000000000004, 'batch_size': 16, 'optimizer': 'SGD', 'weight_decay': 0.00017226104391624577}. Best is trial 2 with value: 0.5338205302515295.\n",
      "[I 2025-06-24 09:12:39,602] Trial 6 finished with value: 0.47824609109449356 and parameters: {'num_hidden_layers': 3, 'neurons_per_layers': 170, 'epochs': 10, 'learning_rate': 3.470870075354676e-05, 'dropout_rate': 0.30000000000000004, 'batch_size': 128, 'optimizer': 'Adam', 'weight_decay': 0.000862987604997892}. Best is trial 2 with value: 0.5338205302515295.\n",
      "[I 2025-06-24 09:13:41,968] Trial 7 finished with value: 0.4326988443235894 and parameters: {'num_hidden_layers': 2, 'neurons_per_layers': 350, 'epochs': 60, 'learning_rate': 0.00017505130753431964, 'dropout_rate': 0.5, 'batch_size': 128, 'optimizer': 'SGD', 'weight_decay': 0.0004588862645910121}. Best is trial 2 with value: 0.5338205302515295.\n",
      "[I 2025-06-24 09:18:14,386] Trial 8 finished with value: 0.44255608429639703 and parameters: {'num_hidden_layers': 4, 'neurons_per_layers': 90, 'epochs': 80, 'learning_rate': 0.018471454741928163, 'dropout_rate': 0.5, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 3.559932457477745e-05}. Best is trial 2 with value: 0.5338205302515295.\n",
      "[I 2025-06-24 09:18:36,184] Trial 9 finished with value: 0.5384092454112849 and parameters: {'num_hidden_layers': 5, 'neurons_per_layers': 350, 'epochs': 10, 'learning_rate': 0.0003748707892803981, 'dropout_rate': 0.1, 'batch_size': 128, 'optimizer': 'Adam', 'weight_decay': 1.271945303049543e-05}. Best is trial 9 with value: 0.5384092454112849.\n"
     ]
    }
   ],
   "source": [
    "study.optimize(objective , n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16867ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
