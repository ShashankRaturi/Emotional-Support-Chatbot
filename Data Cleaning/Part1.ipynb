{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f3f80e",
   "metadata": {},
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808dc9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20e0aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Post_id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>jhc5fc</td>\n",
       "      <td>I’ve wasted so much time being depressed, that...</td>\n",
       "      <td>I can’t imagine how many days I have wasted to...</td>\n",
       "      <td>depressed</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>k47q9a</td>\n",
       "      <td>Anhedonia is the worst part of depression</td>\n",
       "      <td>You're alive, but you aren't living. You feel ...</td>\n",
       "      <td>depressed</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i2h6dv</td>\n",
       "      <td>Life doesn't \"get better\" unless you take acti...</td>\n",
       "      <td>Exercise/physical activity, eating healthy, sp...</td>\n",
       "      <td>depressed</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>g6jgq0</td>\n",
       "      <td>High functioning depression is so easily over ...</td>\n",
       "      <td>I try and do anything and everything all day. ...</td>\n",
       "      <td>depressed</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>kgmy1e</td>\n",
       "      <td>I secretly hope that I will be diagnosed with ...</td>\n",
       "      <td>Hey Reddit. Drunk me here, mainly looking to v...</td>\n",
       "      <td>depressed</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Post_id                                              Title  \\\n",
       "0           0  jhc5fc  I’ve wasted so much time being depressed, that...   \n",
       "1           1  k47q9a          Anhedonia is the worst part of depression   \n",
       "2           2  i2h6dv  Life doesn't \"get better\" unless you take acti...   \n",
       "3           3  g6jgq0  High functioning depression is so easily over ...   \n",
       "4           4  kgmy1e  I secretly hope that I will be diagnosed with ...   \n",
       "\n",
       "                                                Text    emotion   subreddit  \n",
       "0  I can’t imagine how many days I have wasted to...  depressed  depression  \n",
       "1  You're alive, but you aren't living. You feel ...  depressed  depression  \n",
       "2  Exercise/physical activity, eating healthy, sp...  depressed  depression  \n",
       "3  I try and do anything and everything all day. ...  depressed  depression  \n",
       "4  Hey Reddit. Drunk me here, mainly looking to v...  depressed  depression  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('/media/shashanks/3E0CD8C50CD878FB/CDAC/My work/CDAC PROJECTS/MACHINE LEARNING/Emotional-Support-Chatbot/Data Collection/emotions_dataset.csv')\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1d5f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking duplicate posts\n",
    "dataset.duplicated('Post_id').sum() # 0 => no duplicate posts\n",
    "\n",
    "# beacause in the scapping code we mentioned \n",
    "# if post.id in seen_ids:\n",
    "#     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I can’t imagine how many days I have wasted to...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You're alive, but you aren't living. You feel ...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exercise/physical activity, eating healthy, sp...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I try and do anything and everything all day. ...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hey Reddit. Drunk me here, mainly looking to v...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text    emotion\n",
       "0  I can’t imagine how many days I have wasted to...  depressed\n",
       "1  You're alive, but you aren't living. You feel ...  depressed\n",
       "2  Exercise/physical activity, eating healthy, sp...  depressed\n",
       "3  I try and do anything and everything all day. ...  depressed\n",
       "4  Hey Reddit. Drunk me here, mainly looking to v...  depressed"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since our focus is on Text and its emotion \n",
    "# lets pick them \n",
    "\n",
    "df = dataset[['Text' , 'emotion']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018910cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "fearful      4019\n",
       "depressed    3990\n",
       "happy        3834\n",
       "regret       3559\n",
       "angry        3507\n",
       "sad          3441\n",
       "surprised    2605\n",
       "neutral      2261\n",
       "disgusted    2203\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seeing unique emotions\n",
    "df['emotion'].value_counts()\n",
    "\n",
    "# these are the emotions and number of corresponding entries per emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0075fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets convert all this text into lowercase\n",
    "df['Text'] = df['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i can’t imagine how many days i have wasted to...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you're alive, but you aren't living. you feel ...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exercise/physical activity, eating healthy, sp...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i try and do anything and everything all day. ...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hey reddit. drunk me here, mainly looking to v...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text    emotion\n",
       "0  i can’t imagine how many days i have wasted to...  depressed\n",
       "1  you're alive, but you aren't living. you feel ...  depressed\n",
       "2  exercise/physical activity, eating healthy, sp...  depressed\n",
       "3  i try and do anything and everything all day. ...  depressed\n",
       "4  hey reddit. drunk me here, mainly looking to v...  depressed"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d048e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any url from the text if present\n",
    "df['Text'] = df['Text'].apply(lambda sentence : re.sub('\\b(?:https?|ftp|ssh)://\\S+', '' , str(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for extra white spaces\n",
    "df['Text'] = df['Text'].apply(lambda sentence : ' '.join(sentence.split()))\n",
    "\n",
    "# how this is working\n",
    "\n",
    "# sentence = 'this is a   amazing    method  to remove  extra  spaces.    '\n",
    "# words = sentence.split() # ['this', 'is', 'a', 'amazing', 'method', 'to', 'remove', 'extra', 'spaces.']\n",
    "# ' '.join(words) # 'this is a amazing method to remove extra spaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb974f",
   "metadata": {},
   "source": [
    "####  lets do lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1cd0db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords , wordnet\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also we will keep in mind we lemmatize only those words which are not stopwords of english language\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stopwords_eng = stopwords.words('english')\n",
    "corpus = []\n",
    "\n",
    "\n",
    "# defining pos_tag mapper for WordNetLemmatizer as it does not understand default pos_tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "for i in range(len(df)):\n",
    "    \n",
    "    # removing the non alphabetic characters\n",
    "    text_reviewed = re.sub('[^a-zA-Z]' , ' ' , df['Text'][i])\n",
    "    # [^a-zA-Z] => replace anything except alphabets with ' '\n",
    "\n",
    "    # extract words\n",
    "    words = text_reviewed.split()\n",
    "\n",
    "    # lets provide them pos_tag\n",
    "    words_pos_tag = nltk.pos_tag(words)\n",
    "    # it is a list of tuples , where each tuple contains word and its pos tag\n",
    "\n",
    "    # pos_tag gives the tags in the format : NN , VB , VBZ , JJ , but WorNetLemmatizer excepts them in its format.\n",
    "    # it supports only four tags : n , v , a , r\n",
    "\n",
    "    # lemmatize the word if it is not stopword\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word = tup[0] , pos = get_wordnet_pos(tup[1]))\n",
    "                    for tup in words_pos_tag if tup[0] not in stopwords_eng]\n",
    "    \n",
    "    # join these lemmatized words to get back a sentence\n",
    "    corpus.append(' '.join(lemmatized_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8415c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imagine many day waste sad cry able accomplish thing need responsibility deadline none seem matter dark episodes force need do feel hopeless numb',\n",
       " 'alive living feel like something anything nothing appeal everything feel like chore exhaust thing enjoy work since everything feel like watch paint dry realising slowly begin lose interest thing enjoy realise nothing mentally torture like try catch smoke bare hand hopeless thing look forward death',\n",
       " 'exercise physical activity eat healthy spending time nature find hobby thing suppose good mental health definition depression problem precisely inhibit inability function much less thing good find therapist trialling multiple therapist see one suit best even start antidepressant check doctor psychiatrist every week change medication one might fuck head even even work take much fucking effort bother go counsellor university earlier year suggest thing like group therapy anxiety volunteer cause give people sense purpose day day life know tell exactly problem utterly sit office right know anyone possibly way people depression able easily thing help need classify illness disorder infuriating see even professional mental health advocacy organization repeat hackneyed slogans cannot understand even still mass circulation irrational frustrate physically able take ten step house go goddamn walk absolutely mental anything lie bed listen music action do purely passively effort whatsoever get sad start spiralling guilt stop think irrational even though experience many year constantly surprising incompetence ever expect anything different hopelessness feel confused suppose anything even anymore edit thank comment expect get response hard describe feel move people reach small way short moment time',\n",
       " 'try anything everything day alone thought objective much keep busy every evening exhaust emotionally drain still depress feel like beat back relentless force feel like rob proud little thing like wash face baking anyway like proud effort put big fear wake anything head fortunately able eat brush teeth thing every day outsider look like really well struggle lot year stay bed anything thing everyday feel even hopeless proof even proactive life feel shit would fuck',\n",
       " 'hey reddit drunk mainly look vent noone else alt fuck might dox picture meet woman fell love hopelessly love three year later get pregnant young scared tell anything make work baby cry promise baby girl world love never thing change woman live parent longer stay family due covid okay thats fine go store quarantine two week saw someone quarantine two week quickly realize opinion regard baby input sway raise suddenly feel like babysitter allow come work watch baby mom ignore eventually woman fell love start distant little thing first want cuddle stop kiss stop invite thing tell gay decide make work right love woman say want still make work course start antagonize insinuate bad father full time baby dont choice ignore argue heartbroken know else describe help ask something wrong keep tell make work get bad recently fall bad depression life couple suicide attempt past thing stop know would leave baby girl without father dont know donf know post accomplish maybe rant get chest know sure edit barely remember make post afraid look reply awhile glad guy give real good advice thankful thank']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9536d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['emotion'] = encoder.fit_transform(df['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c5e49668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['angry', 'depressed', 'disgusted', 'fearful', 'happy', 'neutral',\n",
       "       'regret', 'sad', 'surprised'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77738215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now its time to divide the data into training and testing set \n",
    "# because we don't want any data leakage by training word2vec on entore corpus first and then splitting\n",
    "\n",
    "# e.g. if we have a test point like : \"I feel really sad.\"\n",
    "# and our word2vec model has already seen similar words or complete phrase while training the entire corpus\n",
    "# then we will be getting very good embeddings , leading to higher accuracy later on\n",
    "# but this accuracy is not the correct accuracy becasue our model already knoew that data , hence lead to overfitting\n",
    "# so to avoid this , lets first split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cd2906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , Y_train , Y_test = train_test_split(df['Text'] , df['emotion'] , train_size=0.8 , random_state=123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c35e8689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i can’t imagine how many days i have wasted to being sad and crying and not being able to accomplish the things i need to do. i have responsibilities i have deadlines and none of them seem to matter when i am in these dark episodes. how can you force yourself to do what needs to be done when you feel so hopeless and numb?\n",
      "you're alive, but you aren't living. you feel like doing something, anything, but nothing is appealing. everything feels like a chore, and it's exhausting. \"do things that you enjoy doing\" doesn't work, since everything feels like watching paint dry. realising that you're slowly beginning to lose interest in the few things you enjoy doing and realising that there's nothing you can do about it is mentally torturing. it's like trying to catch smoke with your bare hands; it's hopeless. the only thing you look forwards to is death.\n",
      "exercise/physical activity, eating healthy, spending time in nature, finding a hobby are all things that are supposed to \"be good for your mental health\"... but by definition, depression is a problem *precisely because* it inhibits your inability to function, much less do things that are good for you. finding a therapist, trialling multiple therapists to see which one suits me best, or even starting on antidepressants and having to check in with a doctor/psychiatrist every few weeks, or having to change medications because one might fuck up my head even more, or not even work... all takes so much fucking effort that i can't bother to do any of it. i went to a counsellor at my university earlier this year, and they suggested things like group therapy for my anxiety and volunteering for causes because that can give people a sense of purpose in day to day life... and i just didn't know how to tell them, that is exactly the problem, *i have utterly no will to do any of that and that's why i'm sitting in your office right now and i don't know what anyone can possibly do about me being this way*. if people with depression were able to so easily do things to help themselves, we wouldn't need to classify it as an 'illness' or 'disorder'. that's why it's so infuriating when we see how even 'professional' mental health advocacy organizations repeat the hackneyed slogans which i cannot understand how they are even still in mass circulation. it's so irrational and frustrating that i'm physically able to take ten steps out of the house to just go on a goddamn walk, but i have absolutely no mental will to do anything but lie in bed. only listening to music is the only action that can be done purely passively with no effort whatsoever, but then i just get sad and start spiralling from guilt. and i can't stop thinking about how irrational it is, even though i've experienced it for so many years. i'm constantly surprising myself from my own incompetence, as if i ever expected anything different. more than hopelessness, i just feel confused as to what i'm supposed to do when i can't will myself to do anything. what even is this anymore? edit: thank you for all the comments. i didn’t expect to get any response. it’s hard to describe, but i feel very moved that there are people out there that i reached in a small way for a short moment of time.\n",
      "i try and do anything and everything all day. i can't be alone with my thoughts so my objective is to do as much as i can to keep busy. every evening i'm exhausted, emotionally drained and still depressed. i feel like i'm beating back a relentless force. i feel like i've been robbed of being proud of the little things i do like washing my face or baking because i do that anyway so it's not like i can be proud of the effort i've put in. my biggest fear is waking up and not having anything to do, just be in my head. fortunately i'm able eat, brush my teeth and do things every day and to the outsider i look like i'm doing really well but i'm struggling a lot. before this there was a year of staying in bed and not doing anything but now that i'm doing things everyday i feel even more hopeless. it's proof that even if i'm proactive in my life i feel as shit as i would if i did fuck all.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/media/shashanks/3E0CD8C50CD878FB/CDAC/My work/env_nlp/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train)):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#index issue\u001b[39;00m\n",
      "File \u001b[0;32m/media/shashanks/3E0CD8C50CD878FB/CDAC/My work/env_nlp/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/media/shashanks/3E0CD8C50CD878FB/CDAC/My work/env_nlp/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/media/shashanks/3E0CD8C50CD878FB/CDAC/My work/env_nlp/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 4"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_train)):\n",
    "    print(X_train[i])\n",
    "#index issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "350d7f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15245    i constantly see people on facebook spamming t...\n",
       "25827    old chinese wisdom :a moral story a farmer in ...\n",
       "6874     back in the early 1990's, i was 14 years old a...\n",
       "3703     i met my ex husband at 15, i fell for him imme...\n",
       "28468    is it possible for me to watermark hundreds of...\n",
       "                               ...                        \n",
       "3640     my (20m) little brother (16m) has suffered fro...\n",
       "3121     i'll just get straight to the point, three mon...\n",
       "20978    i'm going to try laying it a bit bare here. [i...\n",
       "23274    good day everyone, i would like to take your a...\n",
       "6209     my boyfriend is an ra at our university and hi...\n",
       "Name: Text, Length: 23535, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef951a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i constantly see people on facebook spamming t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>old chinese wisdom :a moral story a farmer in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>back in the early 1990's, i was 14 years old a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i met my ex husband at 15, i fell for him imme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is it possible for me to watermark hundreds of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23530</th>\n",
       "      <td>my (20m) little brother (16m) has suffered fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23531</th>\n",
       "      <td>i'll just get straight to the point, three mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23532</th>\n",
       "      <td>i'm going to try laying it a bit bare here. [i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23533</th>\n",
       "      <td>good day everyone, i would like to take your a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23534</th>\n",
       "      <td>my boyfriend is an ra at our university and hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23535 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      i constantly see people on facebook spamming t...\n",
       "1      old chinese wisdom :a moral story a farmer in ...\n",
       "2      back in the early 1990's, i was 14 years old a...\n",
       "3      i met my ex husband at 15, i fell for him imme...\n",
       "4      is it possible for me to watermark hundreds of...\n",
       "...                                                  ...\n",
       "23530  my (20m) little brother (16m) has suffered fro...\n",
       "23531  i'll just get straight to the point, three mon...\n",
       "23532  i'm going to try laying it a bit bare here. [i...\n",
       "23533  good day everyone, i would like to take your a...\n",
       "23534  my boyfriend is an ra at our university and hi...\n",
       "\n",
       "[23535 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.reset_index().drop('index' , axis=1)\n",
    "# lets save it as well and similarly do it for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7162f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index().drop('index' , axis=1)\n",
    "Y_train = Y_train.reset_index().drop('index' , axis=1)\n",
    "X_test = X_test.reset_index().drop('index' , axis=1)\n",
    "Y_test = Y_test.reset_index().drop('index' , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82194e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sometimes you feel all lonely and/or sad and f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in the wake of recent events i've read a lot o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i was crossing the street, looked left and rig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>preface: i’m a big fan of his work (and art in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>so last night i was at a little party and i de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5879</th>\n",
       "      <td>so i’m a gay man. totally at ease with it. i l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>i think about this sometimes during long thund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>a prime example. i came out of welcome to racc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>never ever thought that it was mark hamill tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>start of covid you got worldwide attention for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5884 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text\n",
       "0     sometimes you feel all lonely and/or sad and f...\n",
       "1     in the wake of recent events i've read a lot o...\n",
       "2     i was crossing the street, looked left and rig...\n",
       "3     preface: i’m a big fan of his work (and art in...\n",
       "4     so last night i was at a little party and i de...\n",
       "...                                                 ...\n",
       "5879  so i’m a gay man. totally at ease with it. i l...\n",
       "5880  i think about this sometimes during long thund...\n",
       "5881  a prime example. i came out of welcome to racc...\n",
       "5882  never ever thought that it was mark hamill tha...\n",
       "5883  start of covid you got worldwide attention for...\n",
       "\n",
       "[5884 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db889c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets convert X_train into vector using word2vec\n",
    "# not passing the X_test because we dont want the model to get the idea of test set before hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3baa1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using word2vec to convert text into vector \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bf2c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets train word2vec model from scratch\n",
    "# our corpus right now is -> list of sentences\n",
    "# but Word2Vec requires list of tokenized sentences i.e., list of list of tokens\n",
    "\n",
    "tokenized_sentences = [sentence.split() for sentence in X_train['Text']]\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences , vector_size = 250)\n",
    "\n",
    "# sentences -> list of tokenized sentences\n",
    "# vector_Size = dimensionof vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63027779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'to',\n",
       " 'and',\n",
       " 'the',\n",
       " 'a',\n",
       " 'my',\n",
       " 'of',\n",
       " 'that',\n",
       " 'was',\n",
       " 'in',\n",
       " 'it',\n",
       " 'for',\n",
       " 'you',\n",
       " 'me',\n",
       " 'but',\n",
       " 'is',\n",
       " 'with',\n",
       " 'this',\n",
       " 'have',\n",
       " 'so',\n",
       " 'he',\n",
       " 'she',\n",
       " 'on',\n",
       " 'just',\n",
       " 'be',\n",
       " 'not',\n",
       " 'her',\n",
       " 'at',\n",
       " 'we',\n",
       " 'as',\n",
       " 'about',\n",
       " 'like',\n",
       " 'had',\n",
       " 'or',\n",
       " 'they',\n",
       " 'all',\n",
       " 'if',\n",
       " 'when',\n",
       " \"i'm\",\n",
       " 'what',\n",
       " 'out',\n",
       " 'are',\n",
       " 'because',\n",
       " 'up',\n",
       " 'get',\n",
       " 'do',\n",
       " 'from',\n",
       " 'would',\n",
       " 'your',\n",
       " 'how',\n",
       " 'know',\n",
       " 'people',\n",
       " 'been',\n",
       " 'one',\n",
       " 'an',\n",
       " 'even',\n",
       " 'feel',\n",
       " 'him',\n",
       " 'can',\n",
       " 'his',\n",
       " \"don't\",\n",
       " 'no',\n",
       " 'time',\n",
       " 'who',\n",
       " 'some',\n",
       " 'want',\n",
       " 'more',\n",
       " 'really',\n",
       " 'never',\n",
       " 'will',\n",
       " 'am',\n",
       " 'were',\n",
       " \"it's\",\n",
       " 'there',\n",
       " 'has',\n",
       " 'by',\n",
       " 'them',\n",
       " 'got',\n",
       " 'going',\n",
       " 'being',\n",
       " 'our',\n",
       " 'after',\n",
       " 'then',\n",
       " 'go',\n",
       " 'could',\n",
       " 'think',\n",
       " 'it.',\n",
       " 'me.',\n",
       " 'their',\n",
       " 'told',\n",
       " 'only',\n",
       " 'much',\n",
       " 'i’m',\n",
       " 'into',\n",
       " 'back',\n",
       " 'make',\n",
       " 'still',\n",
       " 'things',\n",
       " 'now',\n",
       " 'other',\n",
       " \"didn't\",\n",
       " 'life',\n",
       " \"i've\",\n",
       " 'over',\n",
       " 'said',\n",
       " 'did',\n",
       " 'something',\n",
       " 'very',\n",
       " 'see',\n",
       " 'any',\n",
       " 'than',\n",
       " 'myself',\n",
       " 'always',\n",
       " 'good',\n",
       " 'made',\n",
       " 'also',\n",
       " 'every',\n",
       " 'day',\n",
       " 'years',\n",
       " 'work',\n",
       " 'way',\n",
       " 'where',\n",
       " 'first',\n",
       " 'take',\n",
       " 'which',\n",
       " 'started',\n",
       " 'went',\n",
       " 'few',\n",
       " 'don’t',\n",
       " 'someone',\n",
       " 'off',\n",
       " 'say',\n",
       " 'it’s',\n",
       " 'wanted',\n",
       " 'while',\n",
       " 'thought',\n",
       " 'why',\n",
       " 'love',\n",
       " \"can't\",\n",
       " 'most',\n",
       " '-',\n",
       " 'before',\n",
       " 'around',\n",
       " 'through',\n",
       " 'felt',\n",
       " 'me,',\n",
       " 'since',\n",
       " 'these',\n",
       " 'lot',\n",
       " 'need',\n",
       " 'tell',\n",
       " 'friends',\n",
       " 'last',\n",
       " 'too',\n",
       " 'ever',\n",
       " 'doing',\n",
       " 'down',\n",
       " 'right',\n",
       " 'getting',\n",
       " 'little',\n",
       " 'many',\n",
       " 'us',\n",
       " 'help',\n",
       " 'everything',\n",
       " 'should',\n",
       " 'anything',\n",
       " 'having',\n",
       " 'those',\n",
       " 'find',\n",
       " 'fucking',\n",
       " 'thing',\n",
       " 'trying',\n",
       " 'new',\n",
       " 'everyone',\n",
       " 'two',\n",
       " 'best',\n",
       " 'same',\n",
       " 'year',\n",
       " 'talk',\n",
       " 'let',\n",
       " 'long',\n",
       " 'come',\n",
       " 'person',\n",
       " 'family',\n",
       " 'asked',\n",
       " 'try',\n",
       " 'actually',\n",
       " 'home',\n",
       " 'it,',\n",
       " 'own',\n",
       " 'keep',\n",
       " 'until',\n",
       " 'bad',\n",
       " 'put',\n",
       " 'better',\n",
       " 'nothing',\n",
       " 'anyone',\n",
       " 'give',\n",
       " 'next',\n",
       " 'without',\n",
       " 'i’ve',\n",
       " 'feeling',\n",
       " 'here',\n",
       " 'mom',\n",
       " 'another',\n",
       " 'came',\n",
       " 'life.',\n",
       " 'took',\n",
       " 'job',\n",
       " 'sure',\n",
       " 'look',\n",
       " 'friend',\n",
       " \"that's\",\n",
       " 'away',\n",
       " 'maybe',\n",
       " 'hard',\n",
       " 'tried',\n",
       " 'parents',\n",
       " \"wasn't\",\n",
       " 'found',\n",
       " 'able',\n",
       " 'start',\n",
       " 'knew',\n",
       " 'didn’t',\n",
       " 'school',\n",
       " 'pretty',\n",
       " 'left',\n",
       " 'days',\n",
       " 'her.',\n",
       " 'point',\n",
       " 'thank',\n",
       " 'kind',\n",
       " 'care',\n",
       " 'months',\n",
       " 'well',\n",
       " 'him.',\n",
       " 'saying',\n",
       " \"doesn't\",\n",
       " 'both',\n",
       " 'makes',\n",
       " 'enough',\n",
       " 'making',\n",
       " 'each',\n",
       " 'live',\n",
       " \"you're\",\n",
       " '*',\n",
       " 'them.',\n",
       " 'happy',\n",
       " 'old',\n",
       " '2',\n",
       " 'time.',\n",
       " 'used',\n",
       " 'call',\n",
       " 'done',\n",
       " 'once',\n",
       " 'end',\n",
       " 'talking',\n",
       " 'almost',\n",
       " \"i'd\",\n",
       " 'probably',\n",
       " 'dad',\n",
       " 'hate',\n",
       " 'decided',\n",
       " \"i'll\",\n",
       " 'stop',\n",
       " 'might',\n",
       " 'house',\n",
       " 'whole',\n",
       " 'working',\n",
       " 'you.',\n",
       " 'night',\n",
       " 'times',\n",
       " 'post',\n",
       " 'part',\n",
       " 'believe',\n",
       " 'that.',\n",
       " 'shit',\n",
       " 'called',\n",
       " \"couldn't\",\n",
       " 'finally',\n",
       " 'its',\n",
       " 'edit:',\n",
       " 'though',\n",
       " 'does',\n",
       " 'looking',\n",
       " 'hours',\n",
       " 'this.',\n",
       " \"he's\",\n",
       " 'again',\n",
       " 'else',\n",
       " 'thinking',\n",
       " 'bit',\n",
       " \"she's\",\n",
       " 'already',\n",
       " 'remember',\n",
       " 'saw',\n",
       " 'money',\n",
       " 'up.',\n",
       " 'social',\n",
       " 'hope',\n",
       " 'guy',\n",
       " 'understand',\n",
       " '3',\n",
       " 'now.',\n",
       " 'out.',\n",
       " 'ask',\n",
       " 'again.',\n",
       " 'big',\n",
       " 'great',\n",
       " 'read',\n",
       " 'happened',\n",
       " 'different',\n",
       " 'past',\n",
       " 'time,',\n",
       " 'use',\n",
       " 'such',\n",
       " 'fuck',\n",
       " 'kids',\n",
       " 'during',\n",
       " 'needed',\n",
       " 'may',\n",
       " 'gave',\n",
       " 'week',\n",
       " 'anxiety',\n",
       " 'can’t',\n",
       " 'day.',\n",
       " 'living',\n",
       " 'taking',\n",
       " 'leave',\n",
       " 'place',\n",
       " 'girl',\n",
       " 'mental',\n",
       " 'sometimes',\n",
       " 'feels',\n",
       " 'mother',\n",
       " 'relationship',\n",
       " 'now,',\n",
       " 'completely',\n",
       " 'wish',\n",
       " 'least',\n",
       " 'high',\n",
       " 'mind',\n",
       " 'phone',\n",
       " 'world',\n",
       " 'telling',\n",
       " 'couple',\n",
       " 'room',\n",
       " 'yourself',\n",
       " 'lost',\n",
       " 'today',\n",
       " 'wife',\n",
       " 'stay',\n",
       " 'real',\n",
       " 'her,',\n",
       " 'reason',\n",
       " 'idea',\n",
       " 'ago',\n",
       " 'this,',\n",
       " 'man',\n",
       " 'later',\n",
       " 'weeks',\n",
       " 'change',\n",
       " 'small',\n",
       " 'kept',\n",
       " 'together',\n",
       " \"isn't\",\n",
       " 'literally',\n",
       " \"there's\",\n",
       " 'car',\n",
       " 'that,',\n",
       " 'wrong',\n",
       " 'move',\n",
       " 'hear',\n",
       " '&#x200b;',\n",
       " 'pay',\n",
       " 'less',\n",
       " 'head',\n",
       " 'myself.',\n",
       " 'become',\n",
       " 'full',\n",
       " 'do.',\n",
       " 'thanks',\n",
       " 'looked',\n",
       " 'life,',\n",
       " 'instead',\n",
       " 'loved',\n",
       " 'alone',\n",
       " 'you,',\n",
       " 'sister',\n",
       " 'you’re',\n",
       " 'met',\n",
       " 'fact',\n",
       " 'husband',\n",
       " 'guys',\n",
       " 'matter',\n",
       " 'mean',\n",
       " 'day,',\n",
       " \"wouldn't\",\n",
       " 'others',\n",
       " 'ended',\n",
       " 'close',\n",
       " 'entire',\n",
       " 'him,',\n",
       " 'nice',\n",
       " 'sorry',\n",
       " 'stuff',\n",
       " 'single',\n",
       " 'that’s',\n",
       " 'guess',\n",
       " 'brother',\n",
       " 'minutes',\n",
       " 'im',\n",
       " 'gets',\n",
       " 'seen',\n",
       " 'asking',\n",
       " 'thoughts',\n",
       " 'spent',\n",
       " 'face',\n",
       " 'comes',\n",
       " 'up,',\n",
       " 'seeing',\n",
       " '5',\n",
       " 'far',\n",
       " 'story',\n",
       " 'moved',\n",
       " 'hurt',\n",
       " 'says',\n",
       " 'sleep',\n",
       " 'scared',\n",
       " 'coming',\n",
       " 'between',\n",
       " 'so,',\n",
       " 'due',\n",
       " 'please',\n",
       " 'show',\n",
       " 'seems',\n",
       " 'often',\n",
       " 'turned',\n",
       " 'anymore.',\n",
       " \"won't\",\n",
       " 'out,',\n",
       " 'years.',\n",
       " 'health',\n",
       " \"they're\",\n",
       " 'reading',\n",
       " 'there.',\n",
       " 'share',\n",
       " 'three',\n",
       " 'depression',\n",
       " 'again,',\n",
       " 'wants',\n",
       " 'second',\n",
       " 'support',\n",
       " 'father',\n",
       " 'talked',\n",
       " 'worked',\n",
       " 'absolutely',\n",
       " 'bed',\n",
       " 'them,',\n",
       " 'experience',\n",
       " 'front',\n",
       " 'became',\n",
       " 'seem',\n",
       " 'hit',\n",
       " 'food',\n",
       " 'realized',\n",
       " 'all.',\n",
       " 'half',\n",
       " '10',\n",
       " 'body',\n",
       " 'basically',\n",
       " '4',\n",
       " 'spend',\n",
       " 'wasn’t',\n",
       " 'quite',\n",
       " 'eventually',\n",
       " 'college',\n",
       " 'honestly',\n",
       " 'realize',\n",
       " 'heart',\n",
       " 'heard',\n",
       " 'stopped',\n",
       " 'pain',\n",
       " 'self',\n",
       " 'especially',\n",
       " 'truly',\n",
       " 'open',\n",
       " 'doesn’t',\n",
       " 'child',\n",
       " '(i',\n",
       " 'work.',\n",
       " 'situation',\n",
       " 'whatever',\n",
       " 'etc.',\n",
       " 'deal',\n",
       " 'rest',\n",
       " 'brain',\n",
       " 'woman',\n",
       " 'morning',\n",
       " 'yet',\n",
       " 'too.',\n",
       " 'on.',\n",
       " 'either',\n",
       " 'month',\n",
       " 'sick',\n",
       " 'women',\n",
       " 'people.',\n",
       " 'sex',\n",
       " 'set',\n",
       " 'usually',\n",
       " 'gonna',\n",
       " 'side',\n",
       " \"haven't\",\n",
       " \"we're\",\n",
       " 'turn',\n",
       " 'well,',\n",
       " 'way.',\n",
       " 'outside',\n",
       " 'break',\n",
       " 'goes',\n",
       " 'fear',\n",
       " 'learn',\n",
       " 'walk',\n",
       " 'to.',\n",
       " 'comments',\n",
       " 'helped',\n",
       " '6',\n",
       " 'under',\n",
       " 'important',\n",
       " 'moment',\n",
       " 'feelings',\n",
       " 'kid',\n",
       " 'play',\n",
       " 'constantly',\n",
       " 'hour',\n",
       " 'longer',\n",
       " 'eat',\n",
       " 'broke',\n",
       " 'against',\n",
       " 'well.',\n",
       " 'gone',\n",
       " 'better.',\n",
       " 'extremely',\n",
       " 'means',\n",
       " 'normal',\n",
       " 'home.',\n",
       " '&',\n",
       " 'angry',\n",
       " 'fun',\n",
       " 'towards',\n",
       " 'inside',\n",
       " 'back.',\n",
       " 'edit',\n",
       " 'reddit',\n",
       " 'giving',\n",
       " 'along',\n",
       " 'worst',\n",
       " 'free',\n",
       " 'sent',\n",
       " 'stupid',\n",
       " 'friends,',\n",
       " 'gotten',\n",
       " 'problem',\n",
       " 'bring',\n",
       " 'given',\n",
       " 'several',\n",
       " 'tired',\n",
       " 'taken',\n",
       " 'sit',\n",
       " 'using',\n",
       " 'worse',\n",
       " 'seemed',\n",
       " 'course',\n",
       " 'group',\n",
       " 'boyfriend',\n",
       " 'however,',\n",
       " 'rather',\n",
       " 'crying',\n",
       " 'behind',\n",
       " 'soon',\n",
       " 'must',\n",
       " 'i’d',\n",
       " 'watch',\n",
       " 'sort',\n",
       " 'huge',\n",
       " 'daughter',\n",
       " 'meet',\n",
       " 'dont',\n",
       " 'knowing',\n",
       " 'super',\n",
       " 'door',\n",
       " 'he’s',\n",
       " 'cause',\n",
       " 'sad',\n",
       " 'words',\n",
       " 'needs',\n",
       " 'cannot',\n",
       " 'video',\n",
       " 'despite',\n",
       " 'young',\n",
       " 'name',\n",
       " 'off.',\n",
       " 'simply',\n",
       " 'amazing',\n",
       " 'immediately',\n",
       " 'sitting',\n",
       " 'knows',\n",
       " 'baby',\n",
       " 'afraid',\n",
       " 'i’ll',\n",
       " 'enjoy',\n",
       " 'happen',\n",
       " 'son',\n",
       " 'issues',\n",
       " 'friends.',\n",
       " 'girlfriend',\n",
       " 'supposed',\n",
       " 'ago,',\n",
       " 'learned',\n",
       " 'company',\n",
       " 'top',\n",
       " 'us.',\n",
       " 'anything.',\n",
       " 'panic',\n",
       " 'work,',\n",
       " 'weird',\n",
       " 'sense',\n",
       " 'recently',\n",
       " 'years,',\n",
       " 'in.',\n",
       " 'watching',\n",
       " 'write',\n",
       " 'age',\n",
       " 'people,',\n",
       " '\"i',\n",
       " 'married',\n",
       " 'here.',\n",
       " 'advice',\n",
       " 'miss',\n",
       " 'walked',\n",
       " 'cut',\n",
       " 'much.',\n",
       " 'worth',\n",
       " 'men',\n",
       " 'human',\n",
       " 'couldn’t',\n",
       " 'playing',\n",
       " 'hold',\n",
       " 'starting',\n",
       " 'myself,',\n",
       " 'is,',\n",
       " 'forward',\n",
       " 'did.',\n",
       " 'amount',\n",
       " 'brought',\n",
       " 'there,',\n",
       " 'run',\n",
       " 'lives',\n",
       " 'stand',\n",
       " 'ago.',\n",
       " 'personal',\n",
       " 'eyes',\n",
       " 'control',\n",
       " 'things.',\n",
       " 'girls',\n",
       " 'early',\n",
       " 'ex',\n",
       " 'conversation',\n",
       " 'away.',\n",
       " 'class',\n",
       " 'buy',\n",
       " 'she’s',\n",
       " 'positive',\n",
       " 'nobody',\n",
       " 'late',\n",
       " 'exactly',\n",
       " \"shouldn't\",\n",
       " 'began',\n",
       " 'check',\n",
       " 'children',\n",
       " 'whenever',\n",
       " 'definitely',\n",
       " 'lose',\n",
       " 'older',\n",
       " 'wake',\n",
       " 'hand',\n",
       " 'writing',\n",
       " 'middle',\n",
       " 'pick',\n",
       " 'leaving',\n",
       " 'contact',\n",
       " 'person.',\n",
       " 'state',\n",
       " 'order',\n",
       " 'fast',\n",
       " 'lived',\n",
       " 'imagine',\n",
       " 'expect',\n",
       " 'hell',\n",
       " 'problems',\n",
       " 'barely',\n",
       " 'sat',\n",
       " 'ready',\n",
       " 'school.',\n",
       " 'eating',\n",
       " 'plan',\n",
       " 'speak',\n",
       " 'okay',\n",
       " 'together.',\n",
       " 'dog',\n",
       " 'drive',\n",
       " 'therapy',\n",
       " 'walking',\n",
       " 'waiting',\n",
       " '1',\n",
       " 'school,',\n",
       " 'do,',\n",
       " 'paid',\n",
       " 'chance',\n",
       " 'noticed',\n",
       " 'calling',\n",
       " 'type',\n",
       " 'short',\n",
       " 'takes',\n",
       " 'step',\n",
       " 'kill',\n",
       " 'alone.',\n",
       " 'easy',\n",
       " 'meant',\n",
       " 'said,',\n",
       " 'biggest',\n",
       " 'grateful',\n",
       " 'home,',\n",
       " 'family.',\n",
       " 'within',\n",
       " 'wait',\n",
       " 'then,',\n",
       " 'text',\n",
       " 'appreciate',\n",
       " 'random',\n",
       " \"hadn't\",\n",
       " 'explain',\n",
       " 'year.',\n",
       " 'trust',\n",
       " 'dating',\n",
       " 'changed',\n",
       " 'deserve',\n",
       " 'answer',\n",
       " 'looks',\n",
       " 'emotional',\n",
       " 'there’s',\n",
       " 'night.',\n",
       " 'hospital',\n",
       " 'proud',\n",
       " 'ocd',\n",
       " 'one.',\n",
       " 'police',\n",
       " 'certain',\n",
       " 'clean',\n",
       " 'down.',\n",
       " 'moving',\n",
       " 'attention',\n",
       " 'god',\n",
       " 'empty',\n",
       " 'obviously',\n",
       " 'focus',\n",
       " 'allowed',\n",
       " 'isn’t',\n",
       " 'deep',\n",
       " 'more.',\n",
       " 'physical',\n",
       " 'act',\n",
       " 'fight',\n",
       " 'whether',\n",
       " 'future',\n",
       " 'all,',\n",
       " 'ones',\n",
       " 'happened.',\n",
       " 'is.',\n",
       " 'running',\n",
       " 'job.',\n",
       " '20',\n",
       " 'negative',\n",
       " '(and',\n",
       " 'with.',\n",
       " 'job,',\n",
       " 'horrible',\n",
       " 'stuck',\n",
       " 'upset',\n",
       " 'number',\n",
       " 'everything.',\n",
       " 'fucked',\n",
       " 'kinda',\n",
       " 'boss',\n",
       " 'genuinely',\n",
       " 'mostly',\n",
       " 'was.',\n",
       " 'depressed',\n",
       " 'house.',\n",
       " 'case',\n",
       " 'tells',\n",
       " 'ass',\n",
       " 'bought',\n",
       " 'help.',\n",
       " 'apparently',\n",
       " 'clear',\n",
       " 'multiple',\n",
       " 'know,',\n",
       " 'beautiful',\n",
       " 'continue',\n",
       " 'question',\n",
       " 'across',\n",
       " 'putting',\n",
       " 'know.',\n",
       " 'water',\n",
       " 'difficult',\n",
       " 'yes,',\n",
       " 'game',\n",
       " 'currently',\n",
       " '8',\n",
       " 'happy.',\n",
       " \"aren't\",\n",
       " 'passed',\n",
       " 'straight',\n",
       " 'figure',\n",
       " 'fall',\n",
       " 'news',\n",
       " 'online',\n",
       " 'cry',\n",
       " 'doctor',\n",
       " 'known',\n",
       " 'business',\n",
       " 'stayed',\n",
       " 'message',\n",
       " 'fine',\n",
       " 'they’re',\n",
       " 'similar',\n",
       " '30',\n",
       " 'damn',\n",
       " 'terrible',\n",
       " 'thing.',\n",
       " 'date',\n",
       " 'accept',\n",
       " \"what's\",\n",
       " 'woke',\n",
       " 'way,',\n",
       " 'fell',\n",
       " 'weight',\n",
       " 'not.',\n",
       " 'possible',\n",
       " 'before.',\n",
       " 'today,',\n",
       " 'wrong.',\n",
       " 'today.',\n",
       " 'also,',\n",
       " 'shit.',\n",
       " 'hang',\n",
       " 'year,',\n",
       " 'country',\n",
       " 'office',\n",
       " 'incredibly',\n",
       " 'apartment',\n",
       " 'in,',\n",
       " 'turns',\n",
       " 'drink',\n",
       " '.',\n",
       " 'likely',\n",
       " 'crazy',\n",
       " 'back,',\n",
       " \"you've\",\n",
       " 'word',\n",
       " 'listen',\n",
       " 'be.',\n",
       " 'like,',\n",
       " 'showed',\n",
       " 'later,',\n",
       " 'somehow',\n",
       " 'wanting',\n",
       " 'quickly',\n",
       " 'complete',\n",
       " 'family,',\n",
       " 'send',\n",
       " 'black',\n",
       " 'healthy',\n",
       " 'else.',\n",
       " 'totally',\n",
       " 'things,',\n",
       " 'won’t',\n",
       " 'handle',\n",
       " 'night,',\n",
       " 'birthday',\n",
       " 'questions',\n",
       " 'liked',\n",
       " 'broken',\n",
       " 'ran',\n",
       " 'week.',\n",
       " 'ways',\n",
       " 'finding',\n",
       " 'nearly',\n",
       " 'lack',\n",
       " 'worried',\n",
       " 'process',\n",
       " 'true',\n",
       " 'none',\n",
       " 'worry',\n",
       " 'strong',\n",
       " 'losing',\n",
       " 'therapist',\n",
       " 'growing',\n",
       " 'sound',\n",
       " \"we've\",\n",
       " 'died',\n",
       " 'perfect',\n",
       " 'aware',\n",
       " 'excited',\n",
       " 'mad',\n",
       " 'fully',\n",
       " 'reach',\n",
       " 'learning',\n",
       " 'point.',\n",
       " 'notice',\n",
       " 'die',\n",
       " 'on,',\n",
       " 'point,',\n",
       " '7',\n",
       " 'no,',\n",
       " 'anymore',\n",
       " 'here,',\n",
       " 'although',\n",
       " 'games',\n",
       " 'forget',\n",
       " 'hands',\n",
       " 'actual',\n",
       " 'done.',\n",
       " 'something.',\n",
       " 'themselves',\n",
       " 'update',\n",
       " 'avoid',\n",
       " 'low',\n",
       " 'slowly',\n",
       " 'meeting',\n",
       " 'oh',\n",
       " 'struggling',\n",
       " 'to,',\n",
       " 'happens',\n",
       " 'posted',\n",
       " 'world.',\n",
       " 'except',\n",
       " 'daily',\n",
       " 'stories',\n",
       " 'paying',\n",
       " 'account',\n",
       " 'energy',\n",
       " 'book',\n",
       " 'place.',\n",
       " 'media',\n",
       " 'months.',\n",
       " 'drinking',\n",
       " 'anything,',\n",
       " 'quit',\n",
       " 'simple',\n",
       " 'grew',\n",
       " 'emotions',\n",
       " '15',\n",
       " 'person,',\n",
       " 'comment',\n",
       " 'mentioned',\n",
       " 'thinks',\n",
       " 'music',\n",
       " 'nothing.',\n",
       " \"he'd\",\n",
       " 'wouldn’t',\n",
       " 'managed',\n",
       " 'anyway,',\n",
       " 'internet',\n",
       " 'partner',\n",
       " 'store',\n",
       " 'wanna',\n",
       " 'near',\n",
       " 'afford',\n",
       " 'party',\n",
       " 'yesterday',\n",
       " 'extra',\n",
       " ':)',\n",
       " 'wrote',\n",
       " 'cried',\n",
       " 'starts',\n",
       " 'stress',\n",
       " 'death',\n",
       " 'serious',\n",
       " 'house,',\n",
       " 'hated',\n",
       " '(which',\n",
       " 'glad',\n",
       " 'regret',\n",
       " 'fix',\n",
       " 'younger',\n",
       " 'helping',\n",
       " 'throw',\n",
       " 'good.',\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary of my X_train\n",
    "word2vec_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab6dcff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23535"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab size\n",
    "word2vec_model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d47289c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.74061620e-01, -1.13216209e+00, -2.90534496e+00, -1.31299233e+00,\n",
       "        7.96692848e-01, -1.31234789e+00,  4.20345128e-01,  1.51930153e-01,\n",
       "        6.56283855e-01,  3.03263098e-01,  1.33362556e+00,  6.39973462e-01,\n",
       "       -2.96796679e-01,  1.06594801e+00,  6.27940357e-01,  4.21255268e-02,\n",
       "       -3.64112645e-01,  2.05537021e-01, -4.50473686e-04, -6.17356122e-01,\n",
       "        1.31563678e-01, -2.43091464e+00,  1.36672711e+00, -3.89994204e-01,\n",
       "        3.00560445e-01,  3.80706906e-01, -2.91409403e-01,  5.81164598e-01,\n",
       "        9.00105909e-02,  7.19284356e-01, -1.07735074e+00, -1.41918492e+00,\n",
       "        4.63737547e-01,  4.54261247e-03, -1.24501216e+00, -2.48068929e+00,\n",
       "       -2.80575067e-01, -1.08476905e-02,  6.58872545e-01,  1.36725783e+00,\n",
       "        4.02355343e-01,  4.09904450e-01, -6.94465712e-02,  1.47904277e+00,\n",
       "        1.55295742e+00,  1.20790374e+00,  5.00590682e-01, -9.92810130e-01,\n",
       "       -1.07051122e+00, -6.90518737e-01,  6.48398459e-01, -2.38006949e+00,\n",
       "       -9.32627618e-02, -1.42785072e+00,  1.89151883e-01,  2.48094797e+00,\n",
       "        1.54736400e-01,  8.13124716e-01,  6.50815964e-01,  8.24279368e-01,\n",
       "        3.31052393e-01, -2.16607642e+00,  4.30023879e-01,  1.03627384e+00,\n",
       "       -1.20951188e+00, -5.60346805e-02, -1.79188430e-01, -1.50417340e+00,\n",
       "        2.04563737e+00,  1.09390783e+00, -1.38849068e+00,  3.20288152e-01,\n",
       "       -1.73279977e+00, -1.83704960e+00, -1.41100183e-01, -8.89479816e-01,\n",
       "       -1.28866351e+00,  2.14982271e-01,  1.19913030e+00, -1.81742918e+00,\n",
       "       -1.97835612e+00, -5.21103859e-01, -1.63860822e+00,  1.73153269e+00,\n",
       "       -1.53605747e+00, -6.00221395e-01, -2.10654664e+00, -3.67012382e-01,\n",
       "       -1.61710727e+00,  5.53208768e-01,  6.34372234e-01,  1.76837578e-01,\n",
       "       -8.67386758e-01, -3.35532737e+00, -5.50702035e-01,  2.45045114e+00,\n",
       "       -6.01555035e-02,  7.64123738e-01, -2.32530043e-01, -3.60268474e-01,\n",
       "       -2.88100410e+00, -6.48656487e-01, -5.45904934e-01,  7.57818103e-01,\n",
       "        7.62267590e-01,  7.48179257e-01,  1.10452557e+00,  5.90637028e-02,\n",
       "       -1.27683294e+00, -1.91831934e+00, -9.72698271e-01,  6.66047990e-01,\n",
       "       -7.19864547e-01, -1.80340755e+00, -1.03443968e+00,  3.33799958e-01,\n",
       "       -8.06320786e-01, -6.50142431e-01, -1.58191013e+00, -2.51383297e-02,\n",
       "       -1.14844692e+00, -2.85967022e-01, -1.75386286e+00, -1.17985272e+00,\n",
       "       -1.42949605e+00,  2.53370225e-01,  7.43791401e-01,  2.02063394e+00,\n",
       "        1.44633269e+00, -2.67690480e-01,  1.47178710e-01, -2.31646344e-01,\n",
       "       -7.01417148e-01, -1.33829517e-02,  9.75165009e-01, -2.50007719e-01,\n",
       "       -2.11866617e-01,  8.38528752e-01, -2.08992028e+00, -5.71125269e-01,\n",
       "       -3.04382873e+00, -4.42148238e-01,  8.91765356e-01, -1.41206884e+00,\n",
       "       -9.85205770e-01, -1.73610878e+00, -4.54425007e-01,  2.23781157e+00,\n",
       "       -4.33236271e-01,  1.22698963e+00, -1.19107032e+00, -1.56011984e-01,\n",
       "       -1.87434399e+00, -3.38231653e-01, -2.27981865e-01, -2.27760220e+00,\n",
       "        1.13771796e+00, -1.52718413e+00, -6.39135003e-01, -4.82785493e-01,\n",
       "       -1.89068407e-01,  5.97262204e-01, -9.79374409e-01,  1.56500316e+00,\n",
       "       -1.25292754e+00,  2.11947694e-01, -3.78854811e-01, -1.10208225e+00,\n",
       "       -5.80285072e-01, -6.43529892e-01,  5.88675857e-01,  1.39591146e+00,\n",
       "       -1.91224587e+00,  1.33492732e+00,  9.95696306e-01, -6.54529691e-01,\n",
       "       -2.05330327e-01, -1.48180589e-01, -7.97019064e-01,  1.28511190e+00,\n",
       "        1.75218415e+00, -1.84729779e+00, -6.24861360e-01, -1.40925622e+00,\n",
       "        5.27585149e-01, -2.85533518e-02,  1.65096834e-01, -6.46966994e-01,\n",
       "       -1.37716973e+00, -2.04227924e+00, -1.29389763e+00, -1.14113486e+00,\n",
       "        5.02344787e-01,  6.07274950e-01,  1.31790984e+00,  9.89793241e-01,\n",
       "       -1.34929106e-01,  2.18692517e+00,  1.13286030e+00,  3.28637934e+00,\n",
       "       -1.15521893e-01,  6.18646145e-01, -1.03159082e+00,  8.28284323e-01,\n",
       "       -1.85515094e+00,  6.50647998e-01,  1.37866747e+00, -1.54838133e+00,\n",
       "        6.74282074e-01, -8.87395918e-01,  8.88623238e-01,  2.18793780e-01,\n",
       "       -7.38079190e-01,  6.74561083e-01, -1.22200537e+00,  7.06406832e-01,\n",
       "       -5.32607734e-01,  4.85731393e-01, -3.27357560e-01,  9.39164758e-01,\n",
       "        6.21840596e-01,  9.89701629e-01,  2.68188357e+00,  9.64502215e-01,\n",
       "       -9.99629140e-01,  1.97951496e-01, -4.07542020e-01, -5.24205685e-01,\n",
       "       -8.62722516e-01, -6.83388948e-01,  1.10588002e+00,  1.83252549e+00,\n",
       "        5.91795206e-01, -4.36774381e-02,  1.11707652e+00,  2.27688050e+00,\n",
       "       -2.17810488e+00,  1.25805569e+00,  8.65599215e-01,  1.33399355e+00,\n",
       "       -7.42622614e-02,  9.30814385e-01,  1.79458606e+00,  4.70615551e-02,\n",
       "        8.09020549e-02,  1.37823355e+00, -2.43734615e-03,  3.31168354e-01,\n",
       "       -3.80923718e-01, -2.06872120e-01], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv['call'] # see how our vector of 250 dimensions look like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf5c0f",
   "metadata": {},
   "source": [
    "### Using AvgWord2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7665c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'constantly',\n",
       " 'see',\n",
       " 'people',\n",
       " 'on',\n",
       " 'facebook',\n",
       " 'spamming',\n",
       " 'their',\n",
       " 'little',\n",
       " 'mutt',\n",
       " 'on',\n",
       " 'their',\n",
       " 'page',\n",
       " 'with',\n",
       " 'some',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'bodily',\n",
       " 'waste',\n",
       " 'running',\n",
       " 'down',\n",
       " 'themselves,',\n",
       " 'whether',\n",
       " \"it's\",\n",
       " 'piss,',\n",
       " 'snot',\n",
       " 'or',\n",
       " 'spit.',\n",
       " \"it's\",\n",
       " 'all',\n",
       " 'fucking',\n",
       " 'gross',\n",
       " 'and',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'wanna',\n",
       " 'gag.',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'what',\n",
       " 'they',\n",
       " 'find',\n",
       " 'cute',\n",
       " 'about',\n",
       " 'it.',\n",
       " 'on',\n",
       " 'a',\n",
       " 'related',\n",
       " 'note,',\n",
       " 'birthday',\n",
       " 'cakes.',\n",
       " 'everyone',\n",
       " 'else',\n",
       " 'starts',\n",
       " 'cooing',\n",
       " 'and',\n",
       " 'awwing',\n",
       " 'when',\n",
       " 'the',\n",
       " 'toddler,',\n",
       " 'with',\n",
       " 'a',\n",
       " 'finger',\n",
       " 'ridden',\n",
       " 'with',\n",
       " 'snot',\n",
       " 'and',\n",
       " 'shit',\n",
       " \"that's\",\n",
       " 'probably',\n",
       " 'been',\n",
       " 'in',\n",
       " 'his',\n",
       " 'diaper',\n",
       " 'for',\n",
       " '2',\n",
       " 'hours,',\n",
       " 'starts',\n",
       " 'smearing',\n",
       " 'his',\n",
       " 'finger',\n",
       " 'in',\n",
       " 'the',\n",
       " 'cake',\n",
       " 'that',\n",
       " 'everyone',\n",
       " 'else',\n",
       " 'is',\n",
       " 'gonna',\n",
       " 'eat.',\n",
       " 'no',\n",
       " 'repercussions,',\n",
       " 'no',\n",
       " 'consequences,',\n",
       " 'everyone',\n",
       " 'just',\n",
       " 'happily',\n",
       " 'eats',\n",
       " 'the',\n",
       " 'disgusting',\n",
       " 'cake.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why ?\n",
    "\n",
    "# because\n",
    "tokenized_sentences[0]\n",
    "\n",
    "# right now each word is getting converted into a vector which has 100 dimension\n",
    "# if we want the entire sentence to get converted into 1 single vector of 100 dimension , then we will take the avg.\n",
    "# i-th dimension of resultant vector will be avg of i-th dimension of all the vectors of individual words \n",
    "# resultant_vector[i] = avg(v1[i] , v2[i] , .....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5818fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def avg_word2vec(document):\n",
    "\n",
    "    vocab = word2vec_model.wv.index_to_key\n",
    "\n",
    "    # if the word is in vocab only then create its vector\n",
    "    # creating a list of vectors of all the words in a sentence/document\n",
    "    vectors_document = [word2vec_model.wv[word] for word in document if word in vocab]\n",
    "\n",
    "    # converting it to array\n",
    "    array_document = np.array(vectors_document)\n",
    "\n",
    "    if not vectors_document:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    \n",
    "    # otherwise return the mean vector\n",
    "    return np.mean(array_document , axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09951c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23535/23535 [14:02<00:00, 27.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# to see the progress\n",
    "from tqdm import tqdm\n",
    "\n",
    "X_train_new = []\n",
    "# applying avg_word2vec on entire sentence\n",
    "for sentence in tqdm(tokenized_sentences):\n",
    "    X_train_new.append(avg_word2vec(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2aab5e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23535"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "025d4568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.79912674e-01, -5.90954423e-01,  4.45225537e-01, -3.72245580e-01,\n",
       "        2.64145378e-02, -4.10711437e-01,  2.30983160e-02,  2.60745257e-01,\n",
       "       -3.48573066e-02,  1.90578818e-01,  1.03033692e-01,  2.72665489e-02,\n",
       "       -3.47765833e-01, -6.07445687e-02, -8.55252668e-02, -1.53688252e-01,\n",
       "        1.08806111e-01,  4.95494492e-02,  8.84844884e-02,  5.35102822e-02,\n",
       "        1.55950531e-01,  2.29943588e-01,  4.64942344e-02, -3.86003591e-02,\n",
       "       -1.83471441e-01,  1.91023961e-01, -1.32702023e-01, -2.59376168e-01,\n",
       "       -1.70553476e-01,  3.89798462e-01, -1.92828238e-01,  1.54492795e-01,\n",
       "       -2.35888913e-01, -1.70701534e-01,  4.50831838e-02, -1.78148702e-01,\n",
       "       -1.03662601e-02,  5.06278202e-02,  1.46593302e-01, -2.56665975e-01,\n",
       "        3.92356098e-01,  1.01448558e-01, -1.35214135e-01,  2.51872391e-01,\n",
       "        2.83934295e-01, -9.45845917e-02,  4.53401096e-02,  1.52713344e-01,\n",
       "        1.80990338e-01, -2.98866332e-02, -2.65645295e-01,  8.65659937e-02,\n",
       "       -8.85044932e-02,  1.21818602e-01,  2.12373599e-01, -2.33991325e-01,\n",
       "       -1.67730898e-02, -6.56170696e-02, -6.53974637e-02, -4.64633182e-02,\n",
       "        1.47679597e-02, -4.27931249e-02,  3.55831273e-02, -5.57613224e-02,\n",
       "       -2.27326527e-02,  3.93458083e-02, -6.10906780e-01, -1.04080833e-01,\n",
       "       -2.05358356e-01,  2.56809086e-01,  4.48403805e-02,  4.03621018e-01,\n",
       "        1.60485394e-02, -2.98842639e-02,  1.46333491e-02,  1.30817965e-01,\n",
       "       -3.10684629e-02, -2.62236506e-01, -3.47703462e-04,  2.92703360e-02,\n",
       "        1.52965740e-03, -1.10749509e-02,  5.37705002e-03, -1.03352487e-01,\n",
       "       -5.91189042e-02, -1.55474305e-01, -2.45810136e-01,  1.68736577e-01,\n",
       "        1.13435522e-01, -1.05008036e-01,  3.59979495e-02,  1.38318330e-01,\n",
       "       -3.09310257e-02, -3.87153149e-01,  1.26401365e-01,  3.95567328e-01,\n",
       "        1.16837569e-01, -1.72193319e-01,  8.02484751e-02, -6.93813413e-02,\n",
       "        7.35786557e-02,  6.34969547e-02,  4.06447388e-02,  8.25985521e-02,\n",
       "       -3.22983563e-02,  1.32446080e-01, -6.62844554e-02,  1.54868454e-01,\n",
       "       -6.18914627e-02, -3.35102946e-01, -2.16646090e-01, -1.49673879e-01,\n",
       "        1.70911416e-01, -3.26351263e-02,  2.70919651e-01, -3.02119553e-01,\n",
       "        6.49546757e-02,  1.32852480e-01,  1.44146964e-01, -7.91608766e-02,\n",
       "       -1.74351767e-01,  1.66468695e-02, -1.30208790e-01, -2.25640297e-01,\n",
       "       -5.76234758e-02,  1.64496735e-01, -2.03322589e-01, -1.58951610e-01,\n",
       "        4.75427836e-01, -3.84816192e-02,  6.00230731e-02,  3.06376815e-01,\n",
       "        4.07604069e-01,  9.59959999e-02,  6.50871694e-02,  1.68802589e-01,\n",
       "        8.91867280e-02,  3.09268683e-01, -1.81257632e-02, -6.42771125e-02,\n",
       "        1.01398245e-01, -2.62141407e-01,  1.78741306e-01, -2.42542252e-01,\n",
       "        1.65317222e-01, -2.57663965e-01, -1.02147227e-02,  7.20581831e-03,\n",
       "       -2.98091173e-01, -3.27811241e-02,  6.76268041e-02,  5.22078462e-02,\n",
       "        7.33477175e-02, -1.87435165e-01, -6.14347346e-02, -5.84062748e-02,\n",
       "       -7.48030543e-02, -6.11615442e-02,  8.53708684e-02,  1.23181216e-01,\n",
       "       -1.70666315e-02, -2.76742190e-01,  9.51969773e-02, -6.96291914e-03,\n",
       "       -2.15919539e-01,  6.46183267e-02,  1.00453556e-01,  1.17337205e-01,\n",
       "        1.08755482e-02, -2.38744449e-03, -9.18429121e-02, -2.70336419e-01,\n",
       "       -3.64723429e-02,  3.23276103e-01,  1.11958385e-01, -1.03432141e-01,\n",
       "       -2.88441754e-03, -8.77861232e-02, -4.35798950e-02, -7.43225813e-02,\n",
       "       -9.72791612e-02, -1.74947917e-01,  9.75766182e-02,  1.18363671e-01,\n",
       "        3.35881323e-01,  1.86929449e-01,  2.33284459e-01, -1.26424953e-01,\n",
       "       -5.72545156e-02,  3.75727594e-01, -2.25556225e-01, -1.42933190e-01,\n",
       "       -8.71506408e-02, -2.60070324e-01, -1.23815611e-01, -8.99259672e-02,\n",
       "       -2.50174433e-01, -2.15201646e-01,  2.77292803e-02,  8.98528993e-02,\n",
       "        6.29080534e-02,  3.31430808e-02, -3.33911180e-02, -2.04598173e-01,\n",
       "       -7.49369711e-02, -1.43232867e-01, -5.43996334e-01, -1.02259509e-01,\n",
       "        1.39033377e-01,  8.43885168e-02,  3.25055540e-01,  8.88892710e-02,\n",
       "       -2.03669399e-01,  1.40219390e-01, -1.13650948e-01, -5.83974691e-03,\n",
       "        4.02227372e-01, -2.84499973e-02, -3.39661568e-01,  1.90696895e-01,\n",
       "        9.66460630e-02, -1.78258896e-01,  2.11590260e-01, -1.61743315e-03,\n",
       "       -1.50904700e-01,  1.91886187e-01,  1.20883882e-01,  3.25730592e-01,\n",
       "        2.67023891e-01,  7.93665498e-02, -8.70572701e-02, -5.35074361e-02,\n",
       "        2.31577009e-02,  1.96188793e-01,  2.61463702e-01, -9.10931528e-02,\n",
       "        4.13811356e-02, -1.21157110e-01, -2.54979394e-02,  9.04622674e-02,\n",
       "        3.34884599e-02,  2.56118905e-02,  7.92996809e-02, -4.53153461e-01,\n",
       "       -1.35339126e-01, -2.53433943e-01, -1.18944801e-01,  2.56364346e-01,\n",
       "       -1.66853085e-01, -2.60959536e-01], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new[0] # this is the vector representation of our first word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e64b3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.379913</td>\n",
       "      <td>-0.590954</td>\n",
       "      <td>0.445226</td>\n",
       "      <td>-0.372246</td>\n",
       "      <td>0.026415</td>\n",
       "      <td>-0.410711</td>\n",
       "      <td>0.023098</td>\n",
       "      <td>0.260745</td>\n",
       "      <td>-0.034857</td>\n",
       "      <td>0.190579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>0.025612</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>-0.453153</td>\n",
       "      <td>-0.135339</td>\n",
       "      <td>-0.253434</td>\n",
       "      <td>-0.118945</td>\n",
       "      <td>0.256364</td>\n",
       "      <td>-0.166853</td>\n",
       "      <td>-0.260960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.323991</td>\n",
       "      <td>-0.519815</td>\n",
       "      <td>0.377424</td>\n",
       "      <td>-0.534864</td>\n",
       "      <td>0.066183</td>\n",
       "      <td>-0.452272</td>\n",
       "      <td>-0.073674</td>\n",
       "      <td>0.497239</td>\n",
       "      <td>0.128706</td>\n",
       "      <td>0.358055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059999</td>\n",
       "      <td>0.136055</td>\n",
       "      <td>-0.042111</td>\n",
       "      <td>-0.235980</td>\n",
       "      <td>-0.016412</td>\n",
       "      <td>-0.035141</td>\n",
       "      <td>-0.032346</td>\n",
       "      <td>0.393198</td>\n",
       "      <td>-0.361641</td>\n",
       "      <td>-0.191680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470891</td>\n",
       "      <td>-0.706433</td>\n",
       "      <td>0.280104</td>\n",
       "      <td>-0.504282</td>\n",
       "      <td>0.323466</td>\n",
       "      <td>-0.730467</td>\n",
       "      <td>0.135459</td>\n",
       "      <td>0.460936</td>\n",
       "      <td>0.126155</td>\n",
       "      <td>0.348441</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114902</td>\n",
       "      <td>0.196887</td>\n",
       "      <td>0.016223</td>\n",
       "      <td>-0.576676</td>\n",
       "      <td>-0.036948</td>\n",
       "      <td>0.011567</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.594631</td>\n",
       "      <td>-0.468152</td>\n",
       "      <td>-0.564023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.294158</td>\n",
       "      <td>-0.747437</td>\n",
       "      <td>0.355648</td>\n",
       "      <td>-0.554944</td>\n",
       "      <td>0.252860</td>\n",
       "      <td>-0.670446</td>\n",
       "      <td>0.154853</td>\n",
       "      <td>0.584519</td>\n",
       "      <td>0.148885</td>\n",
       "      <td>0.365580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155857</td>\n",
       "      <td>0.183379</td>\n",
       "      <td>-0.055441</td>\n",
       "      <td>-0.470433</td>\n",
       "      <td>0.056903</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>0.031261</td>\n",
       "      <td>0.483978</td>\n",
       "      <td>-0.470758</td>\n",
       "      <td>-0.591772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.287419</td>\n",
       "      <td>-0.676872</td>\n",
       "      <td>0.378734</td>\n",
       "      <td>-0.631298</td>\n",
       "      <td>-0.011777</td>\n",
       "      <td>-0.682109</td>\n",
       "      <td>0.091714</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.275370</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096478</td>\n",
       "      <td>0.126026</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>-0.760246</td>\n",
       "      <td>0.158944</td>\n",
       "      <td>-0.014274</td>\n",
       "      <td>-0.035016</td>\n",
       "      <td>0.461706</td>\n",
       "      <td>-0.415457</td>\n",
       "      <td>-0.326376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.379913 -0.590954  0.445226 -0.372246  0.026415 -0.410711  0.023098   \n",
       "1  0.323991 -0.519815  0.377424 -0.534864  0.066183 -0.452272 -0.073674   \n",
       "2  0.470891 -0.706433  0.280104 -0.504282  0.323466 -0.730467  0.135459   \n",
       "3  0.294158 -0.747437  0.355648 -0.554944  0.252860 -0.670446  0.154853   \n",
       "4  0.287419 -0.676872  0.378734 -0.631298 -0.011777 -0.682109  0.091714   \n",
       "\n",
       "        7         8         9    ...       240       241       242       243  \\\n",
       "0  0.260745 -0.034857  0.190579  ...  0.033488  0.025612  0.079300 -0.453153   \n",
       "1  0.497239  0.128706  0.358055  ...  0.059999  0.136055 -0.042111 -0.235980   \n",
       "2  0.460936  0.126155  0.348441  ... -0.114902  0.196887  0.016223 -0.576676   \n",
       "3  0.584519  0.148885  0.365580  ... -0.155857  0.183379 -0.055441 -0.470433   \n",
       "4  0.358397  0.000933  0.275370  ... -0.096478  0.126026  0.000288 -0.760246   \n",
       "\n",
       "        244       245       246       247       248       249  \n",
       "0 -0.135339 -0.253434 -0.118945  0.256364 -0.166853 -0.260960  \n",
       "1 -0.016412 -0.035141 -0.032346  0.393198 -0.361641 -0.191680  \n",
       "2 -0.036948  0.011567  0.001991  0.594631 -0.468152 -0.564023  \n",
       "3  0.056903  0.031841  0.031261  0.483978 -0.470758 -0.591772  \n",
       "4  0.158944 -0.014274 -0.035016  0.461706 -0.415457 -0.326376  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let me save my training set\n",
    "training_data = pd.DataFrame(X_train_new)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ed81d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23535"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row represents my vector representation of the corresponding sentence in X_train set\n",
    "# each vector has 250 dimensions\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c52e0bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.379913</td>\n",
       "      <td>-0.590954</td>\n",
       "      <td>0.445226</td>\n",
       "      <td>-0.372246</td>\n",
       "      <td>0.026415</td>\n",
       "      <td>-0.410711</td>\n",
       "      <td>0.023098</td>\n",
       "      <td>0.260745</td>\n",
       "      <td>-0.034857</td>\n",
       "      <td>0.190579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025612</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>-0.453153</td>\n",
       "      <td>-0.135339</td>\n",
       "      <td>-0.253434</td>\n",
       "      <td>-0.118945</td>\n",
       "      <td>0.256364</td>\n",
       "      <td>-0.166853</td>\n",
       "      <td>-0.260960</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.323991</td>\n",
       "      <td>-0.519815</td>\n",
       "      <td>0.377424</td>\n",
       "      <td>-0.534864</td>\n",
       "      <td>0.066183</td>\n",
       "      <td>-0.452272</td>\n",
       "      <td>-0.073674</td>\n",
       "      <td>0.497239</td>\n",
       "      <td>0.128706</td>\n",
       "      <td>0.358055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136055</td>\n",
       "      <td>-0.042111</td>\n",
       "      <td>-0.235980</td>\n",
       "      <td>-0.016412</td>\n",
       "      <td>-0.035141</td>\n",
       "      <td>-0.032346</td>\n",
       "      <td>0.393198</td>\n",
       "      <td>-0.361641</td>\n",
       "      <td>-0.191680</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470891</td>\n",
       "      <td>-0.706433</td>\n",
       "      <td>0.280104</td>\n",
       "      <td>-0.504282</td>\n",
       "      <td>0.323466</td>\n",
       "      <td>-0.730467</td>\n",
       "      <td>0.135459</td>\n",
       "      <td>0.460936</td>\n",
       "      <td>0.126155</td>\n",
       "      <td>0.348441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196887</td>\n",
       "      <td>0.016223</td>\n",
       "      <td>-0.576676</td>\n",
       "      <td>-0.036948</td>\n",
       "      <td>0.011567</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.594631</td>\n",
       "      <td>-0.468152</td>\n",
       "      <td>-0.564023</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.294158</td>\n",
       "      <td>-0.747437</td>\n",
       "      <td>0.355648</td>\n",
       "      <td>-0.554944</td>\n",
       "      <td>0.252860</td>\n",
       "      <td>-0.670446</td>\n",
       "      <td>0.154853</td>\n",
       "      <td>0.584519</td>\n",
       "      <td>0.148885</td>\n",
       "      <td>0.365580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183379</td>\n",
       "      <td>-0.055441</td>\n",
       "      <td>-0.470433</td>\n",
       "      <td>0.056903</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>0.031261</td>\n",
       "      <td>0.483978</td>\n",
       "      <td>-0.470758</td>\n",
       "      <td>-0.591772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.287419</td>\n",
       "      <td>-0.676872</td>\n",
       "      <td>0.378734</td>\n",
       "      <td>-0.631298</td>\n",
       "      <td>-0.011777</td>\n",
       "      <td>-0.682109</td>\n",
       "      <td>0.091714</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.275370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126026</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>-0.760246</td>\n",
       "      <td>0.158944</td>\n",
       "      <td>-0.014274</td>\n",
       "      <td>-0.035016</td>\n",
       "      <td>0.461706</td>\n",
       "      <td>-0.415457</td>\n",
       "      <td>-0.326376</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.379913 -0.590954  0.445226 -0.372246  0.026415 -0.410711  0.023098   \n",
       "1  0.323991 -0.519815  0.377424 -0.534864  0.066183 -0.452272 -0.073674   \n",
       "2  0.470891 -0.706433  0.280104 -0.504282  0.323466 -0.730467  0.135459   \n",
       "3  0.294158 -0.747437  0.355648 -0.554944  0.252860 -0.670446  0.154853   \n",
       "4  0.287419 -0.676872  0.378734 -0.631298 -0.011777 -0.682109  0.091714   \n",
       "\n",
       "          7         8         9  ...       241       242       243       244  \\\n",
       "0  0.260745 -0.034857  0.190579  ...  0.025612  0.079300 -0.453153 -0.135339   \n",
       "1  0.497239  0.128706  0.358055  ...  0.136055 -0.042111 -0.235980 -0.016412   \n",
       "2  0.460936  0.126155  0.348441  ...  0.196887  0.016223 -0.576676 -0.036948   \n",
       "3  0.584519  0.148885  0.365580  ...  0.183379 -0.055441 -0.470433  0.056903   \n",
       "4  0.358397  0.000933  0.275370  ...  0.126026  0.000288 -0.760246  0.158944   \n",
       "\n",
       "        245       246       247       248       249  emotion  \n",
       "0 -0.253434 -0.118945  0.256364 -0.166853 -0.260960        2  \n",
       "1 -0.035141 -0.032346  0.393198 -0.361641 -0.191680        4  \n",
       "2  0.011567  0.001991  0.594631 -0.468152 -0.564023        6  \n",
       "3  0.031841  0.031261  0.483978 -0.470758 -0.591772        1  \n",
       "4 -0.014274 -0.035016  0.461706 -0.415457 -0.326376        5  \n",
       "\n",
       "[5 rows x 251 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets append the Y_train column as well into this dataframe\n",
    "training_data['emotion'] = Y_train['emotion']\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "814b349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save it \n",
    "training_data.to_csv('./output/train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e043932d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.379913</td>\n",
       "      <td>-0.590954</td>\n",
       "      <td>0.445226</td>\n",
       "      <td>-0.372246</td>\n",
       "      <td>0.026415</td>\n",
       "      <td>-0.410711</td>\n",
       "      <td>0.023098</td>\n",
       "      <td>0.260745</td>\n",
       "      <td>-0.034857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025612</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>-0.453153</td>\n",
       "      <td>-0.135339</td>\n",
       "      <td>-0.253434</td>\n",
       "      <td>-0.118945</td>\n",
       "      <td>0.256364</td>\n",
       "      <td>-0.166853</td>\n",
       "      <td>-0.260960</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.323991</td>\n",
       "      <td>-0.519815</td>\n",
       "      <td>0.377424</td>\n",
       "      <td>-0.534864</td>\n",
       "      <td>0.066183</td>\n",
       "      <td>-0.452272</td>\n",
       "      <td>-0.073674</td>\n",
       "      <td>0.497239</td>\n",
       "      <td>0.128706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136055</td>\n",
       "      <td>-0.042111</td>\n",
       "      <td>-0.235980</td>\n",
       "      <td>-0.016412</td>\n",
       "      <td>-0.035141</td>\n",
       "      <td>-0.032346</td>\n",
       "      <td>0.393198</td>\n",
       "      <td>-0.361641</td>\n",
       "      <td>-0.191680</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.470891</td>\n",
       "      <td>-0.706433</td>\n",
       "      <td>0.280104</td>\n",
       "      <td>-0.504282</td>\n",
       "      <td>0.323466</td>\n",
       "      <td>-0.730467</td>\n",
       "      <td>0.135459</td>\n",
       "      <td>0.460936</td>\n",
       "      <td>0.126155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196887</td>\n",
       "      <td>0.016223</td>\n",
       "      <td>-0.576676</td>\n",
       "      <td>-0.036948</td>\n",
       "      <td>0.011567</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.594631</td>\n",
       "      <td>-0.468152</td>\n",
       "      <td>-0.564023</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.294158</td>\n",
       "      <td>-0.747437</td>\n",
       "      <td>0.355648</td>\n",
       "      <td>-0.554944</td>\n",
       "      <td>0.252860</td>\n",
       "      <td>-0.670446</td>\n",
       "      <td>0.154853</td>\n",
       "      <td>0.584519</td>\n",
       "      <td>0.148885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183379</td>\n",
       "      <td>-0.055441</td>\n",
       "      <td>-0.470433</td>\n",
       "      <td>0.056903</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>0.031261</td>\n",
       "      <td>0.483978</td>\n",
       "      <td>-0.470758</td>\n",
       "      <td>-0.591772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.287419</td>\n",
       "      <td>-0.676872</td>\n",
       "      <td>0.378734</td>\n",
       "      <td>-0.631298</td>\n",
       "      <td>-0.011777</td>\n",
       "      <td>-0.682109</td>\n",
       "      <td>0.091714</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126026</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>-0.760246</td>\n",
       "      <td>0.158944</td>\n",
       "      <td>-0.014274</td>\n",
       "      <td>-0.035016</td>\n",
       "      <td>0.461706</td>\n",
       "      <td>-0.415457</td>\n",
       "      <td>-0.326376</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1         2         3         4         5  \\\n",
       "0           0  0.379913 -0.590954  0.445226 -0.372246  0.026415 -0.410711   \n",
       "1           1  0.323991 -0.519815  0.377424 -0.534864  0.066183 -0.452272   \n",
       "2           2  0.470891 -0.706433  0.280104 -0.504282  0.323466 -0.730467   \n",
       "3           3  0.294158 -0.747437  0.355648 -0.554944  0.252860 -0.670446   \n",
       "4           4  0.287419 -0.676872  0.378734 -0.631298 -0.011777 -0.682109   \n",
       "\n",
       "          6         7         8  ...       241       242       243       244  \\\n",
       "0  0.023098  0.260745 -0.034857  ...  0.025612  0.079300 -0.453153 -0.135339   \n",
       "1 -0.073674  0.497239  0.128706  ...  0.136055 -0.042111 -0.235980 -0.016412   \n",
       "2  0.135459  0.460936  0.126155  ...  0.196887  0.016223 -0.576676 -0.036948   \n",
       "3  0.154853  0.584519  0.148885  ...  0.183379 -0.055441 -0.470433  0.056903   \n",
       "4  0.091714  0.358397  0.000933  ...  0.126026  0.000288 -0.760246  0.158944   \n",
       "\n",
       "        245       246       247       248       249  emotion  \n",
       "0 -0.253434 -0.118945  0.256364 -0.166853 -0.260960        2  \n",
       "1 -0.035141 -0.032346  0.393198 -0.361641 -0.191680        4  \n",
       "2  0.011567  0.001991  0.594631 -0.468152 -0.564023        6  \n",
       "3  0.031841  0.031261  0.483978 -0.470758 -0.591772        1  \n",
       "4 -0.014274 -0.035016  0.461706 -0.415457 -0.326376        5  \n",
       "\n",
       "[5 rows x 252 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check it \n",
    "temp = pd.read_csv('./output/train_set.csv')\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cdd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.379913</td>\n",
       "      <td>-0.590954</td>\n",
       "      <td>0.445226</td>\n",
       "      <td>-0.372246</td>\n",
       "      <td>0.026415</td>\n",
       "      <td>-0.410711</td>\n",
       "      <td>0.023098</td>\n",
       "      <td>0.260745</td>\n",
       "      <td>-0.034857</td>\n",
       "      <td>0.190579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025612</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>-0.453153</td>\n",
       "      <td>-0.135339</td>\n",
       "      <td>-0.253434</td>\n",
       "      <td>-0.118945</td>\n",
       "      <td>0.256364</td>\n",
       "      <td>-0.166853</td>\n",
       "      <td>-0.260960</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.323991</td>\n",
       "      <td>-0.519815</td>\n",
       "      <td>0.377424</td>\n",
       "      <td>-0.534864</td>\n",
       "      <td>0.066183</td>\n",
       "      <td>-0.452272</td>\n",
       "      <td>-0.073674</td>\n",
       "      <td>0.497239</td>\n",
       "      <td>0.128706</td>\n",
       "      <td>0.358055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136055</td>\n",
       "      <td>-0.042111</td>\n",
       "      <td>-0.235980</td>\n",
       "      <td>-0.016412</td>\n",
       "      <td>-0.035141</td>\n",
       "      <td>-0.032346</td>\n",
       "      <td>0.393198</td>\n",
       "      <td>-0.361641</td>\n",
       "      <td>-0.191680</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470891</td>\n",
       "      <td>-0.706433</td>\n",
       "      <td>0.280104</td>\n",
       "      <td>-0.504282</td>\n",
       "      <td>0.323466</td>\n",
       "      <td>-0.730467</td>\n",
       "      <td>0.135459</td>\n",
       "      <td>0.460936</td>\n",
       "      <td>0.126155</td>\n",
       "      <td>0.348441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196887</td>\n",
       "      <td>0.016223</td>\n",
       "      <td>-0.576676</td>\n",
       "      <td>-0.036948</td>\n",
       "      <td>0.011567</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.594631</td>\n",
       "      <td>-0.468152</td>\n",
       "      <td>-0.564023</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.294158</td>\n",
       "      <td>-0.747437</td>\n",
       "      <td>0.355648</td>\n",
       "      <td>-0.554944</td>\n",
       "      <td>0.252860</td>\n",
       "      <td>-0.670446</td>\n",
       "      <td>0.154853</td>\n",
       "      <td>0.584519</td>\n",
       "      <td>0.148885</td>\n",
       "      <td>0.365580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183379</td>\n",
       "      <td>-0.055441</td>\n",
       "      <td>-0.470433</td>\n",
       "      <td>0.056903</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>0.031261</td>\n",
       "      <td>0.483978</td>\n",
       "      <td>-0.470758</td>\n",
       "      <td>-0.591772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.287419</td>\n",
       "      <td>-0.676872</td>\n",
       "      <td>0.378734</td>\n",
       "      <td>-0.631298</td>\n",
       "      <td>-0.011777</td>\n",
       "      <td>-0.682109</td>\n",
       "      <td>0.091714</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.275370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126026</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>-0.760246</td>\n",
       "      <td>0.158944</td>\n",
       "      <td>-0.014274</td>\n",
       "      <td>-0.035016</td>\n",
       "      <td>0.461706</td>\n",
       "      <td>-0.415457</td>\n",
       "      <td>-0.326376</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.379913 -0.590954  0.445226 -0.372246  0.026415 -0.410711  0.023098   \n",
       "1  0.323991 -0.519815  0.377424 -0.534864  0.066183 -0.452272 -0.073674   \n",
       "2  0.470891 -0.706433  0.280104 -0.504282  0.323466 -0.730467  0.135459   \n",
       "3  0.294158 -0.747437  0.355648 -0.554944  0.252860 -0.670446  0.154853   \n",
       "4  0.287419 -0.676872  0.378734 -0.631298 -0.011777 -0.682109  0.091714   \n",
       "\n",
       "          7         8         9  ...       241       242       243       244  \\\n",
       "0  0.260745 -0.034857  0.190579  ...  0.025612  0.079300 -0.453153 -0.135339   \n",
       "1  0.497239  0.128706  0.358055  ...  0.136055 -0.042111 -0.235980 -0.016412   \n",
       "2  0.460936  0.126155  0.348441  ...  0.196887  0.016223 -0.576676 -0.036948   \n",
       "3  0.584519  0.148885  0.365580  ...  0.183379 -0.055441 -0.470433  0.056903   \n",
       "4  0.358397  0.000933  0.275370  ...  0.126026  0.000288 -0.760246  0.158944   \n",
       "\n",
       "        245       246       247       248       249  emotion  \n",
       "0 -0.253434 -0.118945  0.256364 -0.166853 -0.260960        2  \n",
       "1 -0.035141 -0.032346  0.393198 -0.361641 -0.191680        4  \n",
       "2  0.011567  0.001991  0.594631 -0.468152 -0.564023        6  \n",
       "3  0.031841  0.031261  0.483978 -0.470758 -0.591772        1  \n",
       "4 -0.014274 -0.035016  0.461706 -0.415457 -0.326376        5  \n",
       "\n",
       "[5 rows x 251 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.drop('Unnamed: 0' , inplace=True , axis =  1)\n",
    "temp.head()\n",
    "\n",
    "# training set is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0276c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_train['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c8b6899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GaussianNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.GaussianNB.html\">?<span>Documentation for GaussianNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GaussianNB()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create a model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# model object\n",
    "model_nb = GaussianNB()\n",
    "\n",
    "# train the model\n",
    "model_nb.fit(X_train_new , Y_train['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979acc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sometimes you feel all lonely and/or sad and f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in the wake of recent events i've read a lot o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i was crossing the street, looked left and rig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>preface: i’m a big fan of his work (and art in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>so last night i was at a little party and i de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5879</th>\n",
       "      <td>so i’m a gay man. totally at ease with it. i l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>i think about this sometimes during long thund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>a prime example. i came out of welcome to racc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>never ever thought that it was mark hamill tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>start of covid you got worldwide attention for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5884 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text\n",
       "0     sometimes you feel all lonely and/or sad and f...\n",
       "1     in the wake of recent events i've read a lot o...\n",
       "2     i was crossing the street, looked left and rig...\n",
       "3     preface: i’m a big fan of his work (and art in...\n",
       "4     so last night i was at a little party and i de...\n",
       "...                                                 ...\n",
       "5879  so i’m a gay man. totally at ease with it. i l...\n",
       "5880  i think about this sometimes during long thund...\n",
       "5881  a prime example. i came out of welcome to racc...\n",
       "5882  never ever thought that it was mark hamill tha...\n",
       "5883  start of covid you got worldwide attention for...\n",
       "\n",
       "[5884 rows x 1 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets do testing now\n",
    "X_test # but X_test is purely text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd22df7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5884/5884 [03:47<00:00, 25.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# now lets use our model Word2Vec to convert text of X_test to vector\n",
    "\n",
    "# because Word2Vec requires list of tokenized sentences\n",
    "X_test_tokens = [sentence.split() for sentence in X_test['Text']]\n",
    "\n",
    "# using function avg_word2vec to do the job , exactly like it did for training set\n",
    "X_test_new = []\n",
    "# applying avg_word2vec on entire sentence\n",
    "for sentence in tqdm(X_test_tokens):\n",
    "    X_test_new.append(avg_word2vec(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5f604b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22710732, -0.41303653,  0.49133152, -0.31259632, -0.18476617,\n",
       "       -0.8872622 , -0.17258127,  0.12168251, -0.24634708, -0.1247347 ,\n",
       "        0.29672343,  0.36078936, -0.16671893, -0.09409644,  0.12877515,\n",
       "       -0.26745996,  0.49016753, -0.21245307,  0.09040833,  0.2812171 ,\n",
       "        0.2891945 ,  0.18651561,  0.25615937,  0.3089376 ,  0.09201322,\n",
       "        0.2603449 ,  0.13275301, -0.18867798,  0.15276839,  0.29056263,\n",
       "        0.32302284, -0.4109478 ,  0.2583388 , -0.2388297 ,  0.56595767,\n",
       "       -0.09781045,  0.1389403 ,  0.416598  ,  0.11626725, -0.06388639,\n",
       "       -0.10223866,  0.2692013 ,  0.0720349 ,  0.19931905,  0.2018698 ,\n",
       "       -0.25639495, -0.0343442 ,  0.5145468 ,  0.06320782,  0.02922744,\n",
       "        0.22642133,  0.23809183, -0.17197163, -0.42150694,  0.27561507,\n",
       "       -0.4487381 , -0.33957475, -0.6030782 ,  0.10871722,  0.1818879 ,\n",
       "        0.12295569, -0.06855067,  0.37379932,  0.1486631 , -0.16836277,\n",
       "       -0.00376937, -0.92815506, -0.09867527, -0.3230955 ,  0.10406226,\n",
       "        0.0642345 ,  0.47142646, -0.4517171 , -0.1136291 ,  0.06241861,\n",
       "        0.05045222,  0.5488992 , -0.24477877,  0.21489191,  0.04926869,\n",
       "       -0.25575167, -0.1744519 , -0.5201618 ,  0.09197558,  0.06718985,\n",
       "       -0.21720001, -0.61840045, -0.2832842 ,  0.21918336, -0.31384832,\n",
       "        0.19708006,  0.10506027, -0.25563282, -0.27493188, -0.30259427,\n",
       "        0.42943102, -0.42573866, -0.52885973,  0.05625947,  0.04965143,\n",
       "       -0.26130515,  0.00308014,  0.308112  ,  0.43530908,  0.04824756,\n",
       "        0.05403623, -0.07405342,  0.13917914, -0.46475106, -0.39902237,\n",
       "        0.0299575 , -0.61001474, -0.23263294, -0.316447  ,  0.48459548,\n",
       "       -0.6030297 ,  0.02817421,  0.20142253, -0.31815705,  0.03632536,\n",
       "       -0.29427895,  0.7401506 , -0.41373017, -0.84733427, -0.11892566,\n",
       "        0.50761   , -0.5824076 , -0.34440964,  0.4422191 , -0.1764661 ,\n",
       "        0.31209606,  0.21964455,  0.4008325 , -0.633272  , -0.31948134,\n",
       "       -0.3760515 ,  0.25975242,  0.04185762,  0.2780171 ,  0.28684637,\n",
       "        0.21986607, -0.04330435,  0.11243382, -0.02016661, -0.07279199,\n",
       "       -0.45355156,  0.3119042 ,  0.5569402 , -0.04395265,  0.05999985,\n",
       "       -0.22799389,  0.34745082, -0.1605789 , -0.05287956,  0.15777053,\n",
       "        0.13075526, -0.55330724,  0.02553806,  0.36723217,  0.6164932 ,\n",
       "       -0.13668972, -0.21730553,  0.33612362,  0.03669711, -0.28912732,\n",
       "       -0.0585629 , -0.11735266,  0.38991678, -0.00967561,  0.42367262,\n",
       "        0.24186447, -0.46228975,  0.2558223 ,  0.36463884,  0.07809192,\n",
       "        0.05392709,  0.24181765, -0.30236188, -0.5275971 , -0.05228215,\n",
       "        0.31376797,  0.21504891,  0.25280893, -0.36674017,  0.6014142 ,\n",
       "       -0.09799143,  0.58432376,  0.36150503, -0.22661425,  0.6620649 ,\n",
       "       -0.1041457 , -0.29193488, -0.14104228, -0.7071699 ,  0.05625171,\n",
       "       -0.22918606, -0.37294745, -0.30768296,  0.65643317, -0.13050736,\n",
       "        0.06178217, -0.04057666, -0.01987008, -0.37084284, -0.12195493,\n",
       "       -0.42549905, -0.7291707 , -0.6063658 , -0.1676818 ,  0.1897157 ,\n",
       "        0.4841155 ,  0.06172388, -0.19294709, -0.22033961,  0.35173398,\n",
       "       -0.17360243,  0.7350575 , -0.19537671, -0.11612383, -0.19063602,\n",
       "        0.07883106, -0.39428955, -0.04935723, -0.14098981,  0.13568336,\n",
       "        0.43475214,  0.19739012,  0.14055371,  0.06138663,  0.5600375 ,\n",
       "        0.13427235, -0.31781673,  0.05213421,  0.20475872,  0.02668599,\n",
       "       -0.40365678, -0.235319  , -0.20477992, -0.29655632,  0.18144444,\n",
       "       -0.33786643,  0.21665525,  0.13721603, -0.8262719 ,  0.372452  ,\n",
       "       -0.03528425, -0.16502997,  0.44850513, -0.5000327 , -0.1670011 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e7578b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets predict\n",
    "Y_pred = model_nb.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79acc43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.12      0.18       695\n",
      "           1       0.27      0.22      0.24       792\n",
      "           2       0.31      0.43      0.36       440\n",
      "           3       0.42      0.25      0.32       812\n",
      "           4       0.41      0.48      0.44       763\n",
      "           5       0.30      0.36      0.33       458\n",
      "           6       0.27      0.77      0.40       715\n",
      "           7       0.36      0.12      0.18       692\n",
      "           8       0.31      0.13      0.18       517\n",
      "\n",
      "    accuracy                           0.32      5884\n",
      "   macro avg       0.34      0.32      0.29      5884\n",
      "weighted avg       0.34      0.32      0.29      5884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test['emotion'] , Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the word2vec model as well for later use , but make sure to create avg_word2vec function\n",
    "# in that file as well , because have used avg_word2vec to train\n",
    "\n",
    "word2vec_model.save(\"./output/word2vec_emotion.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b9409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0558c8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
